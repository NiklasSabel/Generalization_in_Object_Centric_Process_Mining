{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3629febf",
   "metadata": {},
   "source": [
    "# Negative Weighted Events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad07bf88",
   "metadata": {},
   "source": [
    "In this notebook, we adapt the negative events-based measure from van den Broucke et al. from 2014 in the paper: \"Determining Process Model Precision and Generalization with Weighted Artificial Negative Events\" (doi: 10.1109/TKDE.2013.130). <br>\n",
    "With respect to the actual executions of a process, the measure focuses on negative events, where a negative event represents information about activities that were prevented from taking place in the first place. Since they are rarely recorded in reality, we induce them into the log artificially. <br>\n",
    "So, we basically induce all elements that weren't fired at this specific position in the event log. For simplicity, the authors assume that all other events, outside the current event itself, are inserted at each position in the trace. Afterwards, we check which of these events could be fired at the current position in the corresponding position or not. If an event can be fired, we increase a counter for allowed generalizations $AG$ by one, and if not, we increase a counter for allowed generalizations $DG$ by one. <br>\n",
    "The final measure is calculated as follows: <br>\n",
    "For event log $E$ and process model $M$,\n",
    "$$ Generalization (L,M) = AG\\,/\\, (AG+DG).$$\n",
    "One has to be aware, that this is not the complete logic of the paper, which is implemented in a further step by introducing a scoring mechanism. This is done due to the fact that we have to assume the completeness of an event log to induce negative events and this is normally not true in reality, as an event log only represents a subset. The scoring mechanism will allow us to loosen this assumption by only assuming the completeness property on a small window before the execution of the current activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9aeff9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T11:24:44.732695Z",
     "start_time": "2023-04-28T11:24:44.699328Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cd21f2c",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from ocpa.objects.log.importer.ocel import factory as ocel_import_factory\n",
    "from ocpa.algo.discovery.ocpn import algorithm as ocpn_discovery_factory\n",
    "from src.utils import get_happy_path_log, create_flower_model, generate_variant_model\n",
    "from ocpa.objects.log.importer.csv import factory as ocel_import_factory_csv\n",
    "import pickle\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from src.utils import dfs, filter_silent_transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3ab5656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_global_variables_with(group_df, filtered_preceding_events_full, filtered_preceding_events,\n",
    "                            filtered_succeeding_activities_updated, events, silent_transitions,trace_list, suffix_lists,\n",
    "                                          current_trace_index, AG, DG):\n",
    "    # list for all the activities that are enabled, starting from all activities that do not have any preceding activity\n",
    "    enabled = [key for key, value in filtered_preceding_events_full.items() if not value]\n",
    "    # initialise a list of already executed activities in this trace\n",
    "    trace = []\n",
    "    # iterate through each case/process execution\n",
    "    for index, row in group_df.iterrows():\n",
    "        # Get the current negative events based on the current activity to be executed\n",
    "        negative_activities = [x for x in events if x != row['event_activity']]\n",
    "        # it may happen that an activity is not present in the model but nevertheless executed in the log\n",
    "        if row['event_activity'] in enabled:\n",
    "            # check which elements in the negative activity list are enabled outside of the current activity\n",
    "            enabled.remove(row['event_activity'])\n",
    "        # get all the negative events that can not be executed in the process model at the moment\n",
    "        disallowed = [value for value in negative_activities if value not in enabled]\n",
    "        # add activity that has been executed to trace\n",
    "        trace.append(row['event_activity'])\n",
    "        #only look at all other traces and the own trace after the current position\n",
    "        modified_suffix_list = suffix_lists[:current_trace_index] + suffix_lists[current_trace_index+1:]\n",
    "        #cut out the already played trace such that we start only at the position after the current event\n",
    "        cutted_current_trace = trace_list[current_trace_index][len(trace):]\n",
    "        #generate the list of suffixes for the reversed traces for more efficient comparison\n",
    "        reversed_activities_current = cutted_current_trace[::-1]\n",
    "        suffixes_current = [reversed_activities_current[i:] for i in range(len(reversed_activities_current))]\n",
    "        #append the suffixes of the current trace without the position\n",
    "        modified_suffix_list.append(suffixes_current)\n",
    "        #initialise scoring dictionary to save the values\n",
    "        activity_dict = {activity: [1] for activity in negative_activities}\n",
    "        #for each negative event\n",
    "        for activity in negative_activities:\n",
    "            #set the window size first as the lenght of the trace - 1, because the current value has already been added to the trace\n",
    "            window_size = len(trace)-1\n",
    "            #initialise the matching_window\n",
    "            matching_window = 0\n",
    "            if window_size == 0:\n",
    "                #if the matching window is zero, break the loop, because we cannot compute a score (division by zero)\n",
    "                break\n",
    "            #for each trace that is not the original trace\n",
    "            for execution in modified_suffix_list:\n",
    "                #for each reversed suffix in the trace\n",
    "                for suffix in execution:\n",
    "                        #set a counter for the moving window\n",
    "                        l = 1\n",
    "                        #while the counter is smaller than the window size or the remaining trace of the suffix and the values at the corresponding positions are the same\n",
    "                        while (l < min(window_size,len(suffix)-1)) & (trace[len(trace)-l] == suffix[l-1]):\n",
    "                            #increment the matching window by one position and the counter by one\n",
    "                            matching_window = matching_window + 1\n",
    "                            l = l + 1\n",
    "                        #after the loop has been broken calculate the sccore and save it in the activity list\n",
    "                        score = (window_size - matching_window)/window_size\n",
    "                        activity_dict[activity].append(score)\n",
    "        #initialise empty lists to store the minimum value of the lists for each activity\n",
    "        min_values_enabled = []\n",
    "        min_values_negative = []\n",
    "        #loop over all events in the log\n",
    "        for value in negative_activities:\n",
    "            if value in activity_dict:\n",
    "                #compute the minimum value for eacch of the events outside of the current event\n",
    "                min_val = min(activity_dict[value])\n",
    "                #seperate the values that are enabled and disabled at this position\n",
    "                if value in enabled:\n",
    "                    min_values_enabled.append(min_val)\n",
    "                elif value in disallowed:\n",
    "                    min_values_negative.append(min_val)\n",
    "        # update the values of allowed and disallowed generalizations based on the paper logic (incremend by 1-weight))\n",
    "        AG = AG + (len(min_values_enabled)-sum(min_values_enabled))\n",
    "        DG = DG + (len(min_values_negative)-sum(min_values_negative))\n",
    "        # may happen that activities in the log are not in the process model\n",
    "        if row['event_activity'] in filtered_succeeding_activities_updated:\n",
    "            # get all possible new enabled activities\n",
    "            possible_enabled = filtered_succeeding_activities_updated[row['event_activity']]\n",
    "            # check if each activity has more than one directly preceding state\n",
    "            for i in range(len(possible_enabled)):\n",
    "                # check if an event has two or more activities that need to be executed before the event can take place, if not add events to enabled\n",
    "                if len(filtered_preceding_events[possible_enabled[i]]) < 2:\n",
    "                    enabled.append(possible_enabled[i])\n",
    "                # if all succeeding events equal all preceding events, we have a flower model and almost everything is enabled all the time\n",
    "                elif filtered_preceding_events[possible_enabled[i]] == filtered_succeeding_activities_updated[\n",
    "                    possible_enabled[i]]:\n",
    "                    enabled.append(possible_enabled[i])\n",
    "                else:\n",
    "                    # if yes, check if all the needed activities have already been performed in this trace\n",
    "                    if all(elem in trace for elem in filtered_preceding_events[possible_enabled[i]]):\n",
    "                        enabled.append(possible_enabled[i])\n",
    "        # extend the list with all elements that do not have any preceding activity and are therefore enabled anyways in our process model\n",
    "        enabled.extend([key for key, value in filtered_preceding_events_full.items() if not value])\n",
    "        # delete all duplicates from the enabled list\n",
    "        enabled = list(set(enabled))\n",
    "    #increate the current trace rat\n",
    "    current_trace_index = current_trace_index + 1\n",
    "    return AG, DG\n",
    "\n",
    "# Define the function that will be executed in parallel\n",
    "def process_group_with(args):\n",
    "    \"\"\"\n",
    "    Function to process a group of the event log in parallel\n",
    "    :param args: set of variables for the measure calculation (see original function)\n",
    "    :return: updated values for AG and DG, type: int\n",
    "    \"\"\"\n",
    "    group_key, df_group, filtered_preceding_events_full, filtered_preceding_events, \\\n",
    "    filtered_succeeding_activities_updated, events, silent_transitions,trace_list, suffix_lists,current_trace_index, AG, DG = args\n",
    "    AG, DG = update_global_variables_with(df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
    "                                     filtered_succeeding_activities_updated, events, silent_transitions,trace_list, suffix_lists,\n",
    "                                          current_trace_index,AG, DG)\n",
    "    return AG, DG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fd47f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_events_with_weighting_parallel(ocel, ocpn):\n",
    "    \"\"\"\n",
    "    Function to calculate the negative events measure with weighting based on the used places inside an object-centric petri-net.\n",
    "    :param ocel: object-centric event log for which the measure should be calculated, type: ocel-log\n",
    "    :param ocpn: corresponding object-centric petri-net, type: object-centric petri-net\n",
    "    :return generalization: final value of the formula, type: float rounded to 4 digits\n",
    "    \"\"\"\n",
    "    # since the process execution mappings have lists of length one,\n",
    "    # we create another dictionary that only contains the the value inside the list to be able to derive the case\n",
    "    mapping_dict = {key: ocel.process_execution_mappings[key][0] for key in ocel.process_execution_mappings}\n",
    "    # we generate a new column in the class (log) that contains the process execution (case) number via the generated dictionary\n",
    "    ocel.log.log['event_execution'] = ocel.log.log.index.map(mapping_dict)\n",
    "    # generate a list of unique events in the event log\n",
    "    events = np.unique(ocel.log.log.event_activity)\n",
    "    # dictionary to store each activity as key and a list of its prior states/places as value\n",
    "    targets = {}\n",
    "    # dictionary to store each activity as key and a list of its following states/places as value\n",
    "    sources = {}\n",
    "    for arc in tqdm(ocpn.arcs, desc=\"Check the arcs\"):\n",
    "        # for each arc, check if our target is a valid transition\n",
    "        if arc.target in ocpn.transitions:\n",
    "            # load all the prior places of a valid transition into a dictionary, where the key is the transition and the value\n",
    "            # a list of all directly prior places\n",
    "            if arc.target.name in targets:\n",
    "                targets[arc.target.name].append(arc.source.name)\n",
    "            else:\n",
    "                targets[arc.target.name] = [arc.source.name]\n",
    "        if arc.source in ocpn.transitions:\n",
    "            # load all the following places of a valid transition into a dictionary, where the key is the transition and the value\n",
    "            # a list of all directly following places\n",
    "            if arc.source.name in sources:\n",
    "                sources[arc.source.name].append(arc.target.name)\n",
    "            else:\n",
    "                sources[arc.source.name] = [arc.target.name]\n",
    "    # generate an empty dictionary to store the directly preceding transition of an activity\n",
    "    preceding_activities = {}\n",
    "    # use the key and value of targets and source to generate the dictionary\n",
    "    for target_key, target_value in targets.items():\n",
    "        preceding_activities[target_key] = []\n",
    "        for source_key, source_value in sources.items():\n",
    "            for element in target_value:\n",
    "                if element in source_value:\n",
    "                    preceding_activities[target_key].append(source_key)\n",
    "                    break\n",
    "    # generate an empty dictionary to store the directly succeeding transition of an activity\n",
    "    succeeding_activities = {}\n",
    "    for source_key, source_value in sources.items():\n",
    "        succeeding_activities[source_key] = []\n",
    "        for target_key, target_value in targets.items():\n",
    "            for element in source_value:\n",
    "                if element in target_value:\n",
    "                    succeeding_activities[source_key].append(target_key)\n",
    "                    break\n",
    "    # store the name of all silent transitions in the log\n",
    "    silent_transitions = [x.name for x in ocpn.transitions if x.silent]\n",
    "    # replace the silent transitions in the succeeding activities dictionary by creating a new dictionary to store the modified values\n",
    "    succeeding_activities_updated = {}\n",
    "    # Iterate through the dictionary\n",
    "    for key, values in succeeding_activities.items():\n",
    "        # Create a list to store the modified values for this key\n",
    "        new_values = []\n",
    "        # Iterate through the values of each key\n",
    "        for i in range(len(values)):\n",
    "            # Check if the value is in the list of silent transitions\n",
    "            if values[i] in silent_transitions:\n",
    "                # Replace the value with the corresponding value from the dictionary\n",
    "                new_values.extend(succeeding_activities[values[i]])\n",
    "            else:\n",
    "                # If the value is not in the list of silent transitions, add it to the new list\n",
    "                new_values.append(values[i])\n",
    "        # Add the modified values to the new dictionary\n",
    "        succeeding_activities_updated[key] = new_values\n",
    "    # create an empty dictionary to store all the preceding activities of an activity\n",
    "    preceding_events_dict = {}\n",
    "    # use a depth-first search (DFS) algorithm to traverse the activity graph and\n",
    "    # create a list of all preceding events for each activity in the dictionary for directly preceding activities\n",
    "    for activity in preceding_activities:\n",
    "        # empty set for all the visited activities\n",
    "        visited = set()\n",
    "        # list for all currently preceding events\n",
    "        preceding_events = []\n",
    "        dfs(preceding_activities, visited, activity, preceding_events)\n",
    "        # we need to remove the last element from the list because it corresponds to the activity itself\n",
    "        preceding_events_dict[activity] = preceding_events[:-1][::-1]\n",
    "    # delete all possible silent transitions from preceding_events_dict (dict where all direct preceding events are stored)\n",
    "    filtered_preceding_events_full = filter_silent_transitions(preceding_events_dict, silent_transitions)\n",
    "    # delete all possible silent transitions from filtered_preceding_events (dict where only direct preceding events are stored)\n",
    "    filtered_preceding_events = filter_silent_transitions(preceding_activities, silent_transitions)\n",
    "    # delete all possible silent transitions from succeeding_activities_updated (dict where only direct preceding events are stored)\n",
    "    filtered_succeeding_activities_updated = filter_silent_transitions(succeeding_activities_updated,\n",
    "                                                                       silent_transitions)\n",
    "    # generate a grouped df such that we can iterate through the log case by case (sort by timestamp to ensure the correct process sequence)\n",
    "    grouped_df = ocel.log.log.sort_values('event_timestamp').groupby('event_execution')\n",
    "    DG = 0  # Disallowed Generalization initialisation\n",
    "    AG = 0  # Allowed Generalization initialisation\n",
    "    #create an empty list for all traces in the log\n",
    "    trace_list = []\n",
    "    for name, group in grouped_df:\n",
    "        values = list(map(str, group['event_activity']))\n",
    "        trace_list.append(values)\n",
    "    #create an empty list for all the reversed suffixes\n",
    "    suffix_lists = []\n",
    "    for activities in trace_list:\n",
    "        #generate the list of suffixes for the reversed traces for more efficient comparison\n",
    "        reversed_activities = activities[::-1]\n",
    "        suffixes = [reversed_activities[i:] for i in range(len(reversed_activities))]\n",
    "        suffix_lists.append(suffixes)\n",
    "    #Initialise the current trace index\n",
    "    return grouped_df, filtered_preceding_events_full, filtered_preceding_events, filtered_succeeding_activities_updated, events, silent_transitions, trace_list, suffix_lists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5acca2",
   "metadata": {},
   "source": [
    "# O2C Log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f9f1b6",
   "metadata": {},
   "source": [
    "### Standard Petri Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea029b35",
   "metadata": {},
   "source": [
    "In a first step, we load the OCEL-log into the notebook and generate the object-centric petri net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bc28bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../src/data/jsonocel/order_process.jsonocel\"\n",
    "ocel = ocel_import_factory.apply(filename)\n",
    "ocpn = ocpn_discovery_factory.apply(ocel, parameters={\"debug\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8448274",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 46/46 [00:00<00:00, 113226.52it/s]\n",
      "  0%|          | 0/48 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # generate the variables needed for the parallel processing\n",
    "    grouped_df, filtered_preceding_events_full, filtered_preceding_events, filtered_succeeding_activities_updated, events, silent_transitions, trace_list, suffix_lists = negative_events_with_weighting_parallel(\n",
    "        ocel, ocpn)\n",
    "\n",
    "    current_trace_index = 0  # initialise count variable for trace\n",
    "    DG = 0  # Disallowed Generalization initialisation\n",
    "    AG = 0  # Allowed Generalization initialisation\n",
    "\n",
    "    # Create a multiprocessing Pool\n",
    "    pool = multiprocessing.Pool(5)\n",
    "\n",
    "    # Prepare the arguments for parallel processing\n",
    "    args = [(group_key, df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
    "             filtered_succeeding_activities_updated, events, silent_transitions, trace_list, suffix_lists,\n",
    "             current_trace_index, AG, DG)\n",
    "            for group_key, df_group in grouped_df]\n",
    "\n",
    "    # Apply the parallel processing to each group with additional variables\n",
    "    results = []\n",
    "    with tqdm(total=len(grouped_df)) as pbar:\n",
    "        for result in pool.imap_unordered(process_group_with, args):\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate the final sums of AG and DG\n",
    "    final_AG = sum([result[0] for result in results])\n",
    "    final_DG = sum([result[1] for result in results])\n",
    "\n",
    "    # Close the multiprocessing Pool and join the processes\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # calculate the generalization based on the paper\n",
    "    generalization = final_AG / (final_AG + final_DG)\n",
    "    print(np.round(generalization, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a08e25",
   "metadata": {},
   "source": [
    "### Happy Path Petri Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c4f310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_path__ocel = get_happy_path_log(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35668a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_path_ocpn = ocpn_discovery_factory.apply(happy_path__ocel, parameters={\"debug\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8896d9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 26/26 [00:00<?, ?it/s]\n",
      "100%|██████████| 48/48 [00:08<00:00,  5.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3073\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    #generate the variables needed for the parallel processing\n",
    "    grouped_df, filtered_preceding_events_full, filtered_preceding_events, filtered_succeeding_activities_updated, events, silent_transitions = negative_events_without_weighting_parallel(ocel, happy_path_ocpn)\n",
    "\n",
    "    DG = 0  # Disallowed Generalization initialisation\n",
    "    AG = 0  # Allowed Generalization initialisation\n",
    "\n",
    "    # Create a multiprocessing Pool\n",
    "    pool = multiprocessing.Pool(5)\n",
    "\n",
    "    # Prepare the arguments for parallel processing\n",
    "    args = [(group_key, df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
    "             filtered_succeeding_activities_updated, events, silent_transitions, AG, DG)\n",
    "            for group_key, df_group in grouped_df]\n",
    "\n",
    "    \n",
    "    # Apply the parallel processing to each group with additional variables\n",
    "    results = []\n",
    "    with tqdm(total=len(grouped_df)) as pbar:\n",
    "        for result in pool.imap_unordered(process_group_without, args):\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate the final sums of AG and DG\n",
    "    final_AG = sum([result[0] for result in results])\n",
    "    final_DG = sum([result[1] for result in results])\n",
    "\n",
    "    # Close the multiprocessing Pool and join the processes\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # calculate the generalization based on the paper\n",
    "    generalization = final_AG / (final_AG + final_DG)\n",
    "    print(np.round(generalization,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b26a81",
   "metadata": {},
   "source": [
    "### Flower Model Petri Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf08a8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ots = [\"order\",\"item\",\"delivery\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89072314",
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_ocpn = create_flower_model(filename,ots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d01ce1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 32/32 [00:00<00:00, 32040.52it/s]\n",
      "100%|██████████| 48/48 [00:07<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.965\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    #generate the variables needed for the parallel processing\n",
    "    grouped_df, filtered_preceding_events_full, filtered_preceding_events, filtered_succeeding_activities_updated, events, silent_transitions = negative_events_without_weighting_parallel(ocel, flower_ocpn)\n",
    "\n",
    "    DG = 0  # Disallowed Generalization initialisation\n",
    "    AG = 0  # Allowed Generalization initialisation\n",
    "\n",
    "    # Create a multiprocessing Pool\n",
    "    pool = multiprocessing.Pool(5)\n",
    "\n",
    "    # Prepare the arguments for parallel processing\n",
    "    args = [(group_key, df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
    "             filtered_succeeding_activities_updated, events, silent_transitions, AG, DG)\n",
    "            for group_key, df_group in grouped_df]\n",
    "\n",
    "    \n",
    "    # Apply the parallel processing to each group with additional variables\n",
    "    results = []\n",
    "    with tqdm(total=len(grouped_df)) as pbar:\n",
    "        for result in pool.imap_unordered(process_group_without, args):\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate the final sums of AG and DG\n",
    "    final_AG = sum([result[0] for result in results])\n",
    "    final_DG = sum([result[1] for result in results])\n",
    "\n",
    "    # Close the multiprocessing Pool and join the processes\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # calculate the generalization based on the paper\n",
    "    generalization = final_AG / (final_AG + final_DG)\n",
    "    print(np.round(generalization,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999bad08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f998876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ae2024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9df86e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b599d42d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20f126e6",
   "metadata": {},
   "source": [
    "# DS4 Log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018e20d0",
   "metadata": {},
   "source": [
    "### Standard Petri Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c50fd73",
   "metadata": {},
   "source": [
    "In a first step, we load the OCEL-log into the notebook and generate the object-centric petri net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0492c9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../src/data/jsonocel/DS4.jsonocel\"\n",
    "ocel = ocel_import_factory.apply(filename)\n",
    "ocpn = ocpn_discovery_factory.apply(ocel, parameters={\"debug\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3a346fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 364/364 [00:00<00:00, 131591.68it/s]\n",
      "100%|██████████| 14507/14507 [03:22<00:00, 71.46it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3604\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    #generate the variables needed for the parallel processing\n",
    "    grouped_df, filtered_preceding_events_full, filtered_preceding_events, filtered_succeeding_activities_updated, events, silent_transitions = negative_events_without_weighting_parallel(ocel, ocpn)\n",
    "\n",
    "    DG = 0  # Disallowed Generalization initialisation\n",
    "    AG = 0  # Allowed Generalization initialisation\n",
    "\n",
    "    # Create a multiprocessing Pool\n",
    "    pool = multiprocessing.Pool(5)\n",
    "\n",
    "    # Prepare the arguments for parallel processing\n",
    "    args = [(group_key, df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
    "             filtered_succeeding_activities_updated, events, silent_transitions, AG, DG)\n",
    "            for group_key, df_group in grouped_df]\n",
    "\n",
    "    \n",
    "    # Apply the parallel processing to each group with additional variables\n",
    "    results = []\n",
    "    with tqdm(total=len(grouped_df)) as pbar:\n",
    "        for result in pool.imap_unordered(process_group_without, args):\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate the final sums of AG and DG\n",
    "    final_AG = sum([result[0] for result in results])\n",
    "    final_DG = sum([result[1] for result in results])\n",
    "\n",
    "    # Close the multiprocessing Pool and join the processes\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # calculate the generalization based on the paper\n",
    "    generalization = final_AG / (final_AG + final_DG)\n",
    "    print(np.round(generalization,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1437d2",
   "metadata": {},
   "source": [
    "### Happy Path Petri Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4f5ffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_path__ocel = get_happy_path_log(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e524ae81",
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_path_ocpn = ocpn_discovery_factory.apply(happy_path__ocel, parameters={\"debug\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d14a533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 70/70 [00:00<00:00, 35027.59it/s]\n",
      "100%|██████████| 14507/14507 [03:38<00:00, 66.45it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0842\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    #generate the variables needed for the parallel processing\n",
    "    grouped_df, filtered_preceding_events_full, filtered_preceding_events, filtered_succeeding_activities_updated, events, silent_transitions = negative_events_without_weighting_parallel(ocel, happy_path_ocpn)\n",
    "\n",
    "    DG = 0  # Disallowed Generalization initialisation\n",
    "    AG = 0  # Allowed Generalization initialisation\n",
    "\n",
    "    # Create a multiprocessing Pool\n",
    "    pool = multiprocessing.Pool(5)\n",
    "\n",
    "    # Prepare the arguments for parallel processing\n",
    "    args = [(group_key, df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
    "             filtered_succeeding_activities_updated, events, silent_transitions, AG, DG)\n",
    "            for group_key, df_group in grouped_df]\n",
    "\n",
    "    \n",
    "    # Apply the parallel processing to each group with additional variables\n",
    "    results = []\n",
    "    with tqdm(total=len(grouped_df)) as pbar:\n",
    "        for result in pool.imap_unordered(process_group_without, args):\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate the final sums of AG and DG\n",
    "    final_AG = sum([result[0] for result in results])\n",
    "    final_DG = sum([result[1] for result in results])\n",
    "\n",
    "    # Close the multiprocessing Pool and join the processes\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # calculate the generalization based on the paper\n",
    "    generalization = final_AG / (final_AG + final_DG)\n",
    "    print(np.round(generalization,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcccf11",
   "metadata": {},
   "source": [
    "### Flower Model Petri Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c197b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "ots = [\"Payment application\",\"Control summary\",\"Entitlement application\",\"Geo parcel document\",\"Inspection\",\"Reference alignment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f84ac84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_ocpn = create_flower_model(filename,ots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0dd82f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 162/162 [00:00<00:00, 23157.16it/s]\n",
      "100%|██████████| 14507/14507 [03:51<00:00, 62.78it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6885\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    #generate the variables needed for the parallel processing\n",
    "    grouped_df, filtered_preceding_events_full, filtered_preceding_events, filtered_succeeding_activities_updated, events, silent_transitions = negative_events_without_weighting_parallel(ocel, flower_ocpn)\n",
    "\n",
    "    DG = 0  # Disallowed Generalization initialisation\n",
    "    AG = 0  # Allowed Generalization initialisation\n",
    "\n",
    "    # Create a multiprocessing Pool\n",
    "    pool = multiprocessing.Pool(5)\n",
    "\n",
    "    # Prepare the arguments for parallel processing\n",
    "    args = [(group_key, df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
    "             filtered_succeeding_activities_updated, events, silent_transitions, AG, DG)\n",
    "            for group_key, df_group in grouped_df]\n",
    "\n",
    "    \n",
    "    # Apply the parallel processing to each group with additional variables\n",
    "    results = []\n",
    "    with tqdm(total=len(grouped_df)) as pbar:\n",
    "        for result in pool.imap_unordered(process_group_without, args):\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate the final sums of AG and DG\n",
    "    final_AG = sum([result[0] for result in results])\n",
    "    final_DG = sum([result[1] for result in results])\n",
    "\n",
    "    # Close the multiprocessing Pool and join the processes\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # calculate the generalization based on the paper\n",
    "    generalization = final_AG / (final_AG + final_DG)\n",
    "    print(np.round(generalization,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c09b15",
   "metadata": {},
   "source": [
    "### Variant Model Petri Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10a892b",
   "metadata": {},
   "source": [
    "Import the primarily generated variant log for our measure computation, while we generate the variant model with the original log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a19c4497",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_variant = \"../src/data/csv/DS4_variant_log.csv\" \n",
    "object_types = [\"Payment application\",\"Control summary\",\"Entitlement application\",\"Geo parcel document\",\"Inspection\",\"Reference alignment\"]\n",
    "parameters = {\"obj_names\": object_types,\n",
    "              \"val_names\": [],\n",
    "              \"act_name\": \"event_activity\",\n",
    "              \"time_name\": \"event_timestamp\",\n",
    "              \"sep\": \",\"}\n",
    "ocel_variant = ocel_import_factory_csv.apply(file_path=filename_variant, parameters=parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d30e292",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../src/data/csv/DS4_variant_ocpn.pickle\", \"rb\") as file:\n",
    "    variant_ocpn = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17203b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs:   1%|          | 5242/695752 [07:50<17:13:36, 11.13it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[27], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m      2\u001B[0m     \n\u001B[0;32m      3\u001B[0m     \u001B[38;5;66;03m#generate the variables needed for the parallel processing\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m     grouped_df, filtered_preceding_events_full, filtered_preceding_events, filtered_succeeding_activities_updated, events, silent_transitions \u001B[38;5;241m=\u001B[39m \u001B[43mnegative_events_without_weighting_parallel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mocel_variant\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvariant_ocpn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m     DG \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m  \u001B[38;5;66;03m# Disallowed Generalization initialisation\u001B[39;00m\n\u001B[0;32m      7\u001B[0m     AG \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m  \u001B[38;5;66;03m# Allowed Generalization initialisation\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Generalization in Object-Centric Process Mining\\models\\negative_events_measure_parallel.py:105\u001B[0m, in \u001B[0;36mnegative_events_without_weighting_parallel\u001B[1;34m(ocel, ocpn)\u001B[0m\n\u001B[0;32m    103\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    104\u001B[0m         targets[arc\u001B[38;5;241m.\u001B[39mtarget\u001B[38;5;241m.\u001B[39mname] \u001B[38;5;241m=\u001B[39m [arc\u001B[38;5;241m.\u001B[39msource\u001B[38;5;241m.\u001B[39mname]\n\u001B[1;32m--> 105\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43marc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msource\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mocpn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransitions\u001B[49m:\n\u001B[0;32m    106\u001B[0m     \u001B[38;5;66;03m# load all the following places of a valid transition into a dictionary, where the key is the transition and the value\u001B[39;00m\n\u001B[0;32m    107\u001B[0m     \u001B[38;5;66;03m# a list of all directly following places\u001B[39;00m\n\u001B[0;32m    108\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m arc\u001B[38;5;241m.\u001B[39msource\u001B[38;5;241m.\u001B[39mname \u001B[38;5;129;01min\u001B[39;00m sources:\n\u001B[0;32m    109\u001B[0m         sources[arc\u001B[38;5;241m.\u001B[39msource\u001B[38;5;241m.\u001B[39mname]\u001B[38;5;241m.\u001B[39mappend(arc\u001B[38;5;241m.\u001B[39mtarget\u001B[38;5;241m.\u001B[39mname)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Generalization in Object-Centric Process Mining\\venv\\lib\\site-packages\\ocpa\\objects\\oc_petri_net\\obj.py:66\u001B[0m, in \u001B[0;36mObjectCentricPetriNet.Place.__eq__\u001B[1;34m(self, other)\u001B[0m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__eq__\u001B[39m(\u001B[38;5;28mself\u001B[39m, other):\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;66;03m# keep the ID for now in places\u001B[39;00m\n\u001B[1;32m---> 66\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mid\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mid\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    #generate the variables needed for the parallel processing\n",
    "    grouped_df, filtered_preceding_events_full, filtered_preceding_events, filtered_succeeding_activities_updated, events, silent_transitions = negative_events_without_weighting_parallel(ocel_variant, variant_ocpn)\n",
    "\n",
    "    DG = 0  # Disallowed Generalization initialisation\n",
    "    AG = 0  # Allowed Generalization initialisation\n",
    "\n",
    "    # Create a multiprocessing Pool\n",
    "    pool = multiprocessing.Pool(7)\n",
    "\n",
    "    # Prepare the arguments for parallel processing\n",
    "    args = [(group_key, df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
    "             filtered_succeeding_activities_updated, events, silent_transitions, AG, DG)\n",
    "            for group_key, df_group in grouped_df]\n",
    "\n",
    "    \n",
    "    # Apply the parallel processing to each group with additional variables\n",
    "    results = []\n",
    "    with tqdm(total=len(grouped_df)) as pbar:\n",
    "        for result in pool.imap_unordered(process_group_without, args):\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate the final sums of AG and DG\n",
    "    final_AG = sum([result[0] for result in results])\n",
    "    final_DG = sum([result[1] for result in results])\n",
    "\n",
    "    # Close the multiprocessing Pool and join the processes\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # calculate the generalization based on the paper\n",
    "    generalization = final_AG / (final_AG + final_DG)\n",
    "    print(np.round(generalization,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341d5926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a461a124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d59990",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
