{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3629febf",
   "metadata": {},
   "source": [
    "# Negative Weighted Events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad07bf88",
   "metadata": {},
   "source": [
    "In this notebook, we adapt the negative events-based measure from van den Broucke et al. from 2014 in the paper: \"Determining Process Model Precision and Generalization with Weighted Artificial Negative Events\" (doi: 10.1109/TKDE.2013.130). <br>\n",
    "With respect to the actual executions of a process, the measure focuses on negative events, where a negative event represents information about activities that were prevented from taking place in the first place. Since they are rarely recorded in reality, we induce them into the log artificially. <br>\n",
    "So, we basically induce all elements that weren't fired at this specific position in the event log. For simplicity, the authors assume that all other events, outside the current event itself, are inserted at each position in the trace. Afterwards, we check which of these events could be fired at the current position in the corresponding position or not. If an event can be fired, we increase a counter for allowed generalizations $AG$ by one, and if not, we increase a counter for allowed generalizations $DG$ by one. <br>\n",
    "The final measure is calculated as follows: <br>\n",
    "For event log $E$ and process model $M$,\n",
    "$$ Generalization (L,M) = AG\\,/\\, (AG+DG).$$\n",
    "One has to be aware, that this is not the complete logic of the paper, which is implemented in a further step by introducing a scoring mechanism. This is done due to the fact that we have to assume the completeness of an event log to induce negative events and this is normally not true in reality, as an event log only represents a subset. The scoring mechanism will allow us to loosen this assumption by only assuming the completeness property on a small window before the execution of the current activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9aeff9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T11:24:44.732695Z",
     "start_time": "2023-04-28T11:24:44.699328Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cd21f2c",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from ocpa.objects.log.importer.ocel import factory as ocel_import_factory\n",
    "from ocpa.algo.discovery.ocpn import algorithm as ocpn_discovery_factory\n",
    "from src.utils import get_happy_path_log, create_flower_model, generate_variant_model\n",
    "from ocpa.objects.log.importer.csv import factory as ocel_import_factory_csv\n",
    "import pickle\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from models.negative_events_measure_parallel import process_group_without, negative_events_without_weighting_parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5acca2",
   "metadata": {},
   "source": [
    "# O2C Log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f9f1b6",
   "metadata": {},
   "source": [
    "### Standard Petri Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea029b35",
   "metadata": {},
   "source": [
    "In a first step, we load the OCEL-log into the notebook and generate the object-centric petri net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bc28bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../src/data/jsonocel/order_process.jsonocel\"\n",
    "ocel = ocel_import_factory.apply(filename)\n",
    "ocpn = ocpn_discovery_factory.apply(ocel, parameters={\"debug\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8448274",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 46/46 [00:00<?, ?it/s]\n",
      "100%|██████████| 48/48 [00:06<00:00,  6.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5063\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    #generate the variables needed for the parallel processing\n",
    "    grouped_df, filtered_preceding_events_full, filtered_preceding_events, filtered_succeeding_activities_updated, events, silent_transitions = negative_events_without_weighting_parallel(ocel, ocpn)\n",
    "\n",
    "    DG = 0  # Disallowed Generalization initialisation\n",
    "    AG = 0  # Allowed Generalization initialisation\n",
    "\n",
    "    # Create a multiprocessing Pool\n",
    "    pool = multiprocessing.Pool(5)\n",
    "\n",
    "    # Prepare the arguments for parallel processing\n",
    "    args = [(group_key, df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
    "             filtered_succeeding_activities_updated, events, silent_transitions, AG, DG)\n",
    "            for group_key, df_group in grouped_df]\n",
    "\n",
    "    \n",
    "    # Apply the parallel processing to each group with additional variables\n",
    "    results = []\n",
    "    with tqdm(total=len(grouped_df)) as pbar:\n",
    "        for result in pool.imap_unordered(process_group_without, args):\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate the final sums of AG and DG\n",
    "    final_AG = sum([result[0] for result in results])\n",
    "    final_DG = sum([result[1] for result in results])\n",
    "\n",
    "    # Close the multiprocessing Pool and join the processes\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # calculate the generalization based on the paper\n",
    "    generalization = final_AG / (final_AG + final_DG)\n",
    "    print(np.round(generalization,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a08e25",
   "metadata": {},
   "source": [
    "### Happy Path Petri Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c4f310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_path__ocel = get_happy_path_log(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35668a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_path_ocpn = ocpn_discovery_factory.apply(happy_path__ocel, parameters={\"debug\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8896d9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 26/26 [00:00<?, ?it/s]\n",
      "100%|██████████| 48/48 [00:08<00:00,  5.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3073\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    #generate the variables needed for the parallel processing\n",
    "    grouped_df, filtered_preceding_events_full, filtered_preceding_events, filtered_succeeding_activities_updated, events, silent_transitions = negative_events_without_weighting_parallel(ocel, happy_path_ocpn)\n",
    "\n",
    "    DG = 0  # Disallowed Generalization initialisation\n",
    "    AG = 0  # Allowed Generalization initialisation\n",
    "\n",
    "    # Create a multiprocessing Pool\n",
    "    pool = multiprocessing.Pool(5)\n",
    "\n",
    "    # Prepare the arguments for parallel processing\n",
    "    args = [(group_key, df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
    "             filtered_succeeding_activities_updated, events, silent_transitions, AG, DG)\n",
    "            for group_key, df_group in grouped_df]\n",
    "\n",
    "    \n",
    "    # Apply the parallel processing to each group with additional variables\n",
    "    results = []\n",
    "    with tqdm(total=len(grouped_df)) as pbar:\n",
    "        for result in pool.imap_unordered(process_group_without, args):\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate the final sums of AG and DG\n",
    "    final_AG = sum([result[0] for result in results])\n",
    "    final_DG = sum([result[1] for result in results])\n",
    "\n",
    "    # Close the multiprocessing Pool and join the processes\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # calculate the generalization based on the paper\n",
    "    generalization = final_AG / (final_AG + final_DG)\n",
    "    print(np.round(generalization,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b26a81",
   "metadata": {},
   "source": [
    "### Flower Model Petri Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf08a8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ots = [\"order\",\"item\",\"delivery\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89072314",
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_ocpn = create_flower_model(filename,ots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d01ce1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 32/32 [00:00<00:00, 32040.52it/s]\n",
      "100%|██████████| 48/48 [00:07<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.965\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    #generate the variables needed for the parallel processing\n",
    "    grouped_df, filtered_preceding_events_full, filtered_preceding_events, filtered_succeeding_activities_updated, events, silent_transitions = negative_events_without_weighting_parallel(ocel, flower_ocpn)\n",
    "\n",
    "    DG = 0  # Disallowed Generalization initialisation\n",
    "    AG = 0  # Allowed Generalization initialisation\n",
    "\n",
    "    # Create a multiprocessing Pool\n",
    "    pool = multiprocessing.Pool(5)\n",
    "\n",
    "    # Prepare the arguments for parallel processing\n",
    "    args = [(group_key, df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
    "             filtered_succeeding_activities_updated, events, silent_transitions, AG, DG)\n",
    "            for group_key, df_group in grouped_df]\n",
    "\n",
    "    \n",
    "    # Apply the parallel processing to each group with additional variables\n",
    "    results = []\n",
    "    with tqdm(total=len(grouped_df)) as pbar:\n",
    "        for result in pool.imap_unordered(process_group_without, args):\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate the final sums of AG and DG\n",
    "    final_AG = sum([result[0] for result in results])\n",
    "    final_DG = sum([result[1] for result in results])\n",
    "\n",
    "    # Close the multiprocessing Pool and join the processes\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # calculate the generalization based on the paper\n",
    "    generalization = final_AG / (final_AG + final_DG)\n",
    "    print(np.round(generalization,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999bad08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f998876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ae2024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9df86e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b599d42d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20f126e6",
   "metadata": {},
   "source": [
    "# DS4 Log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018e20d0",
   "metadata": {},
   "source": [
    "### Standard Petri Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c50fd73",
   "metadata": {},
   "source": [
    "In a first step, we load the OCEL-log into the notebook and generate the object-centric petri net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0492c9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../src/data/jsonocel/DS4.jsonocel\"\n",
    "ocel = ocel_import_factory.apply(filename)\n",
    "ocpn = ocpn_discovery_factory.apply(ocel, parameters={\"debug\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3a346fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 364/364 [00:00<00:00, 131591.68it/s]\n",
      "100%|██████████| 14507/14507 [03:22<00:00, 71.46it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3604\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    #generate the variables needed for the parallel processing\n",
    "    grouped_df, filtered_preceding_events_full, filtered_preceding_events, filtered_succeeding_activities_updated, events, silent_transitions = negative_events_without_weighting_parallel(ocel, ocpn)\n",
    "\n",
    "    DG = 0  # Disallowed Generalization initialisation\n",
    "    AG = 0  # Allowed Generalization initialisation\n",
    "\n",
    "    # Create a multiprocessing Pool\n",
    "    pool = multiprocessing.Pool(5)\n",
    "\n",
    "    # Prepare the arguments for parallel processing\n",
    "    args = [(group_key, df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
    "             filtered_succeeding_activities_updated, events, silent_transitions, AG, DG)\n",
    "            for group_key, df_group in grouped_df]\n",
    "\n",
    "    \n",
    "    # Apply the parallel processing to each group with additional variables\n",
    "    results = []\n",
    "    with tqdm(total=len(grouped_df)) as pbar:\n",
    "        for result in pool.imap_unordered(process_group_without, args):\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate the final sums of AG and DG\n",
    "    final_AG = sum([result[0] for result in results])\n",
    "    final_DG = sum([result[1] for result in results])\n",
    "\n",
    "    # Close the multiprocessing Pool and join the processes\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # calculate the generalization based on the paper\n",
    "    generalization = final_AG / (final_AG + final_DG)\n",
    "    print(np.round(generalization,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1437d2",
   "metadata": {},
   "source": [
    "### Happy Path Petri Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4f5ffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_path__ocel = get_happy_path_log(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e524ae81",
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_path_ocpn = ocpn_discovery_factory.apply(happy_path__ocel, parameters={\"debug\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d14a533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 70/70 [00:00<00:00, 35027.59it/s]\n",
      "100%|██████████| 14507/14507 [03:38<00:00, 66.45it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0842\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    #generate the variables needed for the parallel processing\n",
    "    grouped_df, filtered_preceding_events_full, filtered_preceding_events, filtered_succeeding_activities_updated, events, silent_transitions = negative_events_without_weighting_parallel(ocel, happy_path_ocpn)\n",
    "\n",
    "    DG = 0  # Disallowed Generalization initialisation\n",
    "    AG = 0  # Allowed Generalization initialisation\n",
    "\n",
    "    # Create a multiprocessing Pool\n",
    "    pool = multiprocessing.Pool(5)\n",
    "\n",
    "    # Prepare the arguments for parallel processing\n",
    "    args = [(group_key, df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
    "             filtered_succeeding_activities_updated, events, silent_transitions, AG, DG)\n",
    "            for group_key, df_group in grouped_df]\n",
    "\n",
    "    \n",
    "    # Apply the parallel processing to each group with additional variables\n",
    "    results = []\n",
    "    with tqdm(total=len(grouped_df)) as pbar:\n",
    "        for result in pool.imap_unordered(process_group_without, args):\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate the final sums of AG and DG\n",
    "    final_AG = sum([result[0] for result in results])\n",
    "    final_DG = sum([result[1] for result in results])\n",
    "\n",
    "    # Close the multiprocessing Pool and join the processes\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # calculate the generalization based on the paper\n",
    "    generalization = final_AG / (final_AG + final_DG)\n",
    "    print(np.round(generalization,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcccf11",
   "metadata": {},
   "source": [
    "### Flower Model Petri Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c197b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "ots = [\"Payment application\",\"Control summary\",\"Entitlement application\",\"Geo parcel document\",\"Inspection\",\"Reference alignment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f84ac84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_ocpn = create_flower_model(filename,ots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0dd82f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 162/162 [00:00<00:00, 23157.16it/s]\n",
      "100%|██████████| 14507/14507 [03:51<00:00, 62.78it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6885\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    #generate the variables needed for the parallel processing\n",
    "    grouped_df, filtered_preceding_events_full, filtered_preceding_events, filtered_succeeding_activities_updated, events, silent_transitions = negative_events_without_weighting_parallel(ocel, flower_ocpn)\n",
    "\n",
    "    DG = 0  # Disallowed Generalization initialisation\n",
    "    AG = 0  # Allowed Generalization initialisation\n",
    "\n",
    "    # Create a multiprocessing Pool\n",
    "    pool = multiprocessing.Pool(5)\n",
    "\n",
    "    # Prepare the arguments for parallel processing\n",
    "    args = [(group_key, df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
    "             filtered_succeeding_activities_updated, events, silent_transitions, AG, DG)\n",
    "            for group_key, df_group in grouped_df]\n",
    "\n",
    "    \n",
    "    # Apply the parallel processing to each group with additional variables\n",
    "    results = []\n",
    "    with tqdm(total=len(grouped_df)) as pbar:\n",
    "        for result in pool.imap_unordered(process_group_without, args):\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate the final sums of AG and DG\n",
    "    final_AG = sum([result[0] for result in results])\n",
    "    final_DG = sum([result[1] for result in results])\n",
    "\n",
    "    # Close the multiprocessing Pool and join the processes\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # calculate the generalization based on the paper\n",
    "    generalization = final_AG / (final_AG + final_DG)\n",
    "    print(np.round(generalization,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c09b15",
   "metadata": {},
   "source": [
    "### Variant Model Petri Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10a892b",
   "metadata": {},
   "source": [
    "Import the primarily generated variant log for our measure computation, while we generate the variant model with the original log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a19c4497",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_variant = \"../src/data/csv/DS4_variant_log.csv\" \n",
    "object_types = [\"Payment application\",\"Control summary\",\"Entitlement application\",\"Geo parcel document\",\"Inspection\",\"Reference alignment\"]\n",
    "parameters = {\"obj_names\": object_types,\n",
    "              \"val_names\": [],\n",
    "              \"act_name\": \"event_activity\",\n",
    "              \"time_name\": \"event_timestamp\",\n",
    "              \"sep\": \",\"}\n",
    "ocel_variant = ocel_import_factory_csv.apply(file_path=filename_variant, parameters=parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d30e292",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../src/data/csv/DS4_variant_ocpn.pickle\", \"rb\") as file:\n",
    "    variant_ocpn = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17203b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs:   1%|          | 5242/695752 [07:50<17:13:36, 11.13it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m      2\u001b[0m     \n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m#generate the variables needed for the parallel processing\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     grouped_df, filtered_preceding_events_full, filtered_preceding_events, filtered_succeeding_activities_updated, events, silent_transitions \u001b[38;5;241m=\u001b[39m \u001b[43mnegative_events_without_weighting_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mocel_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariant_ocpn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     DG \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Disallowed Generalization initialisation\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     AG \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Allowed Generalization initialisation\u001b[39;00m\n",
      "File \u001b[1;32m~\\PycharmProjects\\Generalization in Object-Centric Process Mining\\models\\negative_events_measure_parallel.py:105\u001b[0m, in \u001b[0;36mnegative_events_without_weighting_parallel\u001b[1;34m(ocel, ocpn)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    104\u001b[0m         targets[arc\u001b[38;5;241m.\u001b[39mtarget\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m [arc\u001b[38;5;241m.\u001b[39msource\u001b[38;5;241m.\u001b[39mname]\n\u001b[1;32m--> 105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43marc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mocpn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransitions\u001b[49m:\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;66;03m# load all the following places of a valid transition into a dictionary, where the key is the transition and the value\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;66;03m# a list of all directly following places\u001b[39;00m\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arc\u001b[38;5;241m.\u001b[39msource\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01min\u001b[39;00m sources:\n\u001b[0;32m    109\u001b[0m         sources[arc\u001b[38;5;241m.\u001b[39msource\u001b[38;5;241m.\u001b[39mname]\u001b[38;5;241m.\u001b[39mappend(arc\u001b[38;5;241m.\u001b[39mtarget\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[1;32m~\\PycharmProjects\\Generalization in Object-Centric Process Mining\\venv\\lib\\site-packages\\ocpa\\objects\\oc_petri_net\\obj.py:66\u001b[0m, in \u001b[0;36mObjectCentricPetriNet.Place.__eq__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# keep the ID for now in places\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mid\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    #generate the variables needed for the parallel processing\n",
    "    grouped_df, filtered_preceding_events_full, filtered_preceding_events, filtered_succeeding_activities_updated, events, silent_transitions = negative_events_without_weighting_parallel(ocel_variant, variant_ocpn)\n",
    "\n",
    "    DG = 0  # Disallowed Generalization initialisation\n",
    "    AG = 0  # Allowed Generalization initialisation\n",
    "\n",
    "    # Create a multiprocessing Pool\n",
    "    pool = multiprocessing.Pool(7)\n",
    "\n",
    "    # Prepare the arguments for parallel processing\n",
    "    args = [(group_key, df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
    "             filtered_succeeding_activities_updated, events, silent_transitions, AG, DG)\n",
    "            for group_key, df_group in grouped_df]\n",
    "\n",
    "    \n",
    "    # Apply the parallel processing to each group with additional variables\n",
    "    results = []\n",
    "    with tqdm(total=len(grouped_df)) as pbar:\n",
    "        for result in pool.imap_unordered(process_group_without, args):\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate the final sums of AG and DG\n",
    "    final_AG = sum([result[0] for result in results])\n",
    "    final_DG = sum([result[1] for result in results])\n",
    "\n",
    "    # Close the multiprocessing Pool and join the processes\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # calculate the generalization based on the paper\n",
    "    generalization = final_AG / (final_AG + final_DG)\n",
    "    print(np.round(generalization,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341d5926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a461a124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d59990",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
