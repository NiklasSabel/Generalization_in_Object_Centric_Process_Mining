{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3629febf",
   "metadata": {},
   "source": [
    "# Negative Weighted Events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad07bf88",
   "metadata": {},
   "source": [
    "In this notebook, we adapt the negative events-based measure from van den Broucke et al. from 2014 in the paper: \"Determining Process Model Precision and Generalization with Weighted Artificial Negative Events\" (doi: 10.1109/TKDE.2013.130). <br>\n",
    "With respect to the actual executions of a process, the measure focuses on negative events, where a negative event represents information about activities that were prevented from taking place in the first place. Since they are rarely recorded in reality, we induce them into the log artificially. <br>\n",
    "So, we basically induce all elements that weren't fired at this specific position in the event log. For simplicity, the authors assume that all other events, outside the current event itself, are inserted at each position in the trace. Afterwards, we check which of these events could be fired at the current position in the corresponding position or not. If an event can be fired, we increase a counter for allowed generalizations $AG$ by one, and if not, we increase a counter for allowed generalizations $DG$ by one. <br>\n",
    "The final measure is calculated as follows: <br>\n",
    "For event log $E$ and process model $M$,\n",
    "$$ Generalization (L,M) = AG\\,/\\, (AG+DG).$$ <br>\n",
    "\n",
    "In this notebook, we also include the weighting approach from the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57127366",
   "metadata": {},
   "source": [
    "### Everything in this notebook that is not fully executed was run on the servers in Karlsruhe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cd21f2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T10:16:47.286454Z",
     "start_time": "2023-05-12T10:16:36.473777Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from ocpa.objects.log.importer.ocel import factory as ocel_import_factory\n",
    "from ocpa.algo.discovery.ocpn import algorithm as ocpn_discovery_factory\n",
    "from src.utils import get_happy_path_log, create_flower_model, generate_variant_model\n",
    "from ocpa.objects.log.importer.csv import factory as ocel_import_factory_csv\n",
    "from models.negative_events_measure import negative_events_with_weighting\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5acca2",
   "metadata": {},
   "source": [
    "# O2C Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87f49d3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T10:16:48.789559Z",
     "start_time": "2023-05-12T10:16:46.966163Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Variant Models: 100%|██████████| 12/12 [00:04<00:00,  2.69it/s]\n",
      "Processing Variant Nets: 100%|██████████| 12/12 [00:00<00:00, 2219.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########Start generating Object-Centric Petri Net#########\n",
      "#########Finished generating Object-Centric Petri Net#########\n"
     ]
    }
   ],
   "source": [
    "filename = \"../src/data/jsonocel/order_process.jsonocel\"\n",
    "ocel = ocel_import_factory.apply(filename)\n",
    "ocpn = ocpn_discovery_factory.apply(ocel, parameters={\"debug\": False})\n",
    "happy_path__ocel = get_happy_path_log(filename)\n",
    "happy_path_ocpn = ocpn_discovery_factory.apply(happy_path__ocel, parameters={\"debug\": False})\n",
    "ots = [\"order\",\"item\",\"delivery\"]\n",
    "flower_model_ocpn = create_flower_model(filename,ots)\n",
    "filename_variant = \"../src/data/csv/order_process_variant_log.csv\" \n",
    "parameters = {\"obj_names\": ots,\n",
    "              \"val_names\": [],\n",
    "              \"act_name\": \"event_activity\",\n",
    "              \"time_name\": \"event_timestamp\",\n",
    "              \"sep\": \",\"}\n",
    "ocel_variant = ocel_import_factory_csv.apply(file_path=filename_variant, parameters=parameters)\n",
    "variant_ocpn = generate_variant_model(ocel,save_path_logs='../src/data/csv/order_process_variants/order_variant',object_types = ots,save_path_visuals=f\"../reports/figures/order_variant_total.svg\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9e15bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 46/46 [00:00<00:00, 45893.91it/s]\n",
      "Calculate Generalization for all process executions: 100%|██████████| 48/48 [01:55<00:00,  2.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allowed: 149921.8158, Disallowed: 176731.859, Generalization 0.459 for ocpn model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generalization, AG, DG = negative_events_with_weighting(ocel,ocpn)\n",
    "print(f'Allowed: {AG}, Disallowed: {DG}, Generalization {generalization} for ocpn model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "296bb09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 26/26 [00:00<00:00, 23914.89it/s]\n",
      "Calculate Generalization for all process executions: 100%|██████████| 48/48 [01:45<00:00,  2.19s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allowed: 68379.9245, Disallowed: 258273.7504, Generalization 0.2093 for happy path model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generalization, AG, DG = negative_events_with_weighting(ocel,happy_path_ocpn)\n",
    "print(f'Allowed: {AG}, Disallowed: {DG}, Generalization {generalization} for happy path model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3ff4fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 32/32 [00:00<00:00, 31857.99it/s]\n",
      "Calculate Generalization for all process executions: 100%|██████████| 48/48 [01:48<00:00,  2.27s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allowed: 302371.1176, Disallowed: 24282.5573, Generalization 0.9257 for flower model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generalization, AG, DG = negative_events_with_weighting(ocel,flower_model_ocpn)\n",
    "print(f'Allowed: {AG}, Disallowed: {DG}, Generalization {generalization} for flower model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54968043",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 378/378 [00:00<00:00, 15136.98it/s]\n",
      "Calculate Generalization for all process executions: 100%|██████████| 48/48 [06:40<00:00,  8.34s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allowed: 50597.391, Disallowed: 339933.4536, Generalization 0.1296 for variant model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generalization, AG, DG = negative_events_with_weighting(ocel_variant,variant_ocpn)\n",
    "print(f'Allowed: {AG}, Disallowed: {DG}, Generalization {generalization} for variant model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaab8829",
   "metadata": {},
   "source": [
    "# P2P Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "425aa3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Variant Models: 100%|██████████| 20/20 [00:01<00:00, 10.54it/s]\n",
      "Processing Variant Nets: 100%|██████████| 20/20 [00:00<00:00, 8121.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########Start generating Object-Centric Petri Net#########\n",
      "#########Finished generating Object-Centric Petri Net#########\n"
     ]
    }
   ],
   "source": [
    "filename = \"../src/data/jsonocel/p2p-normal.jsonocel\"\n",
    "ocel = ocel_import_factory.apply(filename)\n",
    "ocpn = ocpn_discovery_factory.apply(ocel, parameters={\"debug\": False})\n",
    "happy_path__ocel = get_happy_path_log(filename)\n",
    "happy_path_ocpn = ocpn_discovery_factory.apply(happy_path__ocel, parameters={\"debug\": False})\n",
    "ots = [\"PURCHORD\",\"INVOICE\",\"PURCHREQ\",\"MATERIAL\",\"GDSRCPT\"]\n",
    "flower_model_ocpn = create_flower_model(filename,ots)\n",
    "filename_variant = \"../src/data/csv/p2p_variant_log.csv\" \n",
    "parameters = {\"obj_names\": ots,\n",
    "              \"val_names\": [],\n",
    "              \"act_name\": \"event_activity\",\n",
    "              \"time_name\": \"event_timestamp\",\n",
    "              \"sep\": \",\"}\n",
    "ocel_variant = ocel_import_factory_csv.apply(file_path=filename_variant, parameters=parameters)\n",
    "variant_ocpn = generate_variant_model(ocel,save_path_logs='../src/data/csv/p2p-normal_variants/p2p_variant',object_types = ots ,save_path_visuals=f\"../reports/figures/p2p_variant_total.svg\" ,save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa35f222",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 40/40 [00:00<?, ?it/s]\n",
      "Calculate Generalization for all process executions: 100%|██████████| 80/80 [00:02<00:00, 32.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allowed: 28168.981, Disallowed: 107407.4762, Generalization 0.2078 for ocpn model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generalization, AG, DG = negative_events_with_weighting(ocel,ocpn)\n",
    "print(f'Allowed: {AG}, Disallowed: {DG}, Generalization {generalization} for ocpn model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa914642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 38/38 [00:00<00:00, 38002.75it/s]\n",
      "Calculate Generalization for all process executions: 100%|██████████| 80/80 [00:02<00:00, 28.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allowed: 28811.6095, Disallowed: 106764.8476, Generalization 0.2125 for happy path model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generalization, AG, DG = negative_events_with_weighting(ocel,happy_path_ocpn)\n",
    "print(f'Allowed: {AG}, Disallowed: {DG}, Generalization {generalization} for happy path model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2b13ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 38/38 [00:00<00:00, 38148.29it/s]\n",
      "Calculate Generalization for all process executions: 100%|██████████| 80/80 [00:02<00:00, 28.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allowed: 135576.4571, Disallowed: 0, Generalization 1.0 for flower model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generalization, AG, DG = negative_events_with_weighting(ocel,flower_model_ocpn)\n",
    "print(f'Allowed: {AG}, Disallowed: {DG}, Generalization {generalization} for flower model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff696739",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 760/760 [00:00<00:00, 20595.25it/s]\n",
      "Calculate Generalization for all process executions: 100%|██████████| 80/80 [00:53<00:00,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allowed: 18063.2333, Disallowed: 137865.7976, Generalization 0.1158 for variant model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generalization, AG, DG = negative_events_with_weighting(ocel_variant,variant_ocpn)\n",
    "print(f'Allowed: {AG}, Disallowed: {DG}, Generalization {generalization} for variant model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e9f825",
   "metadata": {},
   "source": [
    "# Run Multiprocessing on server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121759a3",
   "metadata": {},
   "source": [
    "### Everything from here on was run on the servers in Karlsruhe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bec092a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from ocpa.objects.log.importer.ocel import factory as ocel_import_factory\n",
    "from ocpa.algo.discovery.ocpn import algorithm as ocpn_discovery_factory\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fd120e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_silent_transitions(dic,silent_transitions):\n",
    "    \"\"\"\n",
    "    Function to filter out the silent transitions defined by a list from a given dictionary.\n",
    "    :param dic: dictionary to be filtered, type: dictionary\n",
    "    :param silent_transitions: list of silent transitions in an ocel log, type: list\n",
    "    :return updated_dictionary: filtered dictionary, type: dictionary\n",
    "    \"\"\"\n",
    "    updated_dictionary = {}\n",
    "    for key, values in dic.items():\n",
    "        if key not in silent_transitions:\n",
    "            new_values = [val for val in values if val not in silent_transitions]\n",
    "            updated_dictionary[key] = new_values\n",
    "    return updated_dictionary\n",
    "\n",
    "#recursive implementation of a depth-first search (DFS) algorithm\n",
    "def dfs(graph, visited, activity, preceding_events):\n",
    "    \"\"\"\n",
    "    Function to perform a depth-first search (DFS) algorithm on the activity graph.\n",
    "    :param graph: activity graph, type: dictionary\n",
    "    :param visited: set of already visited nodes, type: set\n",
    "    :param activity: current activity, type: string\n",
    "    :param preceding_events: list to store the preceding events, type: list\n",
    "    \"\"\"\n",
    "    #takes as input the activity graph (represented as a dictionary), a set of visited nodes, the current activity, and a list to store the preceding events.\n",
    "    visited.add(activity)\n",
    "    for preceding_event in graph[activity]:\n",
    "        #eighboring activity has not been visited yet, the algorithm visits it by calling the dfs function with the neighboring activity as the current activity.\n",
    "        if preceding_event not in visited:\n",
    "            dfs(graph, visited, preceding_event, preceding_events)\n",
    "    preceding_events.append(activity)\n",
    "\n",
    "def update_global_variables_with(group_df, filtered_preceding_events_full, filtered_preceding_events,\n",
    "                            filtered_succeeding_activities_updated, events, silent_transitions,trace_list, suffix_lists,\n",
    "                                          current_trace_index, AG, DG):\n",
    "    # list for all the activities that are enabled, starting from all activities that do not have any preceding activity\n",
    "    enabled = [key for key, value in filtered_preceding_events_full.items() if not value]\n",
    "    # initialise a list of already executed activities in this trace\n",
    "    trace = []\n",
    "    # iterate through each case/process execution\n",
    "    for index, row in group_df.iterrows():\n",
    "        # Get the current negative events based on the current activity to be executed\n",
    "        negative_activities = [x for x in events if x != row['event_activity']]\n",
    "        # it may happen that an activity is not present in the model but nevertheless executed in the log\n",
    "        if row['event_activity'] in enabled:\n",
    "            # check which elements in the negative activity list are enabled outside of the current activity\n",
    "            enabled.remove(row['event_activity'])\n",
    "        # get all the negative events that can not be executed in the process model at the moment\n",
    "        disallowed = [value for value in negative_activities if value not in enabled]\n",
    "        # add activity that has been executed to trace\n",
    "        trace.append(row['event_activity'])\n",
    "        #only look at all other traces and the own trace after the current position\n",
    "        modified_suffix_list = suffix_lists[:current_trace_index] + suffix_lists[current_trace_index+1:]\n",
    "        #cut out the already played trace such that we start only at the position after the current event\n",
    "        cutted_current_trace = trace_list[current_trace_index][len(trace):]\n",
    "        #generate the list of suffixes for the reversed traces for more efficient comparison\n",
    "        reversed_activities_current = cutted_current_trace[::-1]\n",
    "        suffixes_current = [reversed_activities_current[i:] for i in range(len(reversed_activities_current))]\n",
    "        #append the suffixes of the current trace without the position\n",
    "        modified_suffix_list.append(suffixes_current)\n",
    "        #initialise scoring dictionary to save the values\n",
    "        activity_dict = {activity: [1] for activity in negative_activities}\n",
    "        #for each negative event\n",
    "        for activity in negative_activities:\n",
    "            #set the window size first as the lenght of the trace - 1, because the current value has already been added to the trace\n",
    "            window_size = len(trace)-1\n",
    "            #initialise the matching_window\n",
    "            matching_window = 0\n",
    "            if window_size == 0:\n",
    "                #if the matching window is zero, break the loop, because we cannot compute a score (division by zero)\n",
    "                break\n",
    "            #for each trace that is not the original trace\n",
    "            for execution in modified_suffix_list:\n",
    "                #for each reversed suffix in the trace\n",
    "                for suffix in execution:\n",
    "                        #set a counter for the moving window\n",
    "                        l = 1\n",
    "                        #while the counter is smaller than the window size or the remaining trace of the suffix and the values at the corresponding positions are the same\n",
    "                        while (l < min(window_size,len(suffix)-1)) & (trace[len(trace)-l] == suffix[l-1]):\n",
    "                            #increment the matching window by one position and the counter by one\n",
    "                            matching_window = matching_window + 1\n",
    "                            l = l + 1\n",
    "                        #after the loop has been broken calculate the sccore and save it in the activity list\n",
    "                        score = (window_size - matching_window)/window_size\n",
    "                        activity_dict[activity].append(score)\n",
    "        #initialise empty lists to store the minimum value of the lists for each activity\n",
    "        min_values_enabled = []\n",
    "        min_values_negative = []\n",
    "        #loop over all events in the log\n",
    "        for value in negative_activities:\n",
    "            if value in activity_dict:\n",
    "                #compute the minimum value for eacch of the events outside of the current event\n",
    "                min_val = min(activity_dict[value])\n",
    "                #seperate the values that are enabled and disabled at this position\n",
    "                if value in enabled:\n",
    "                    min_values_enabled.append(min_val)\n",
    "                elif value in disallowed:\n",
    "                    min_values_negative.append(min_val)\n",
    "        # update the values of allowed and disallowed generalizations based on the paper logic (incremend by 1-weight))\n",
    "        AG = AG + (len(min_values_enabled)-sum(min_values_enabled))\n",
    "        DG = DG + (len(min_values_negative)-sum(min_values_negative))\n",
    "        # may happen that activities in the log are not in the process model\n",
    "        if row['event_activity'] in filtered_succeeding_activities_updated:\n",
    "            # get all possible new enabled activities\n",
    "            possible_enabled = filtered_succeeding_activities_updated[row['event_activity']]\n",
    "            # check if each activity has more than one directly preceding state\n",
    "            for i in range(len(possible_enabled)):\n",
    "                # check if an event has two or more activities that need to be executed before the event can take place, if not add events to enabled\n",
    "                if len(filtered_preceding_events[possible_enabled[i]]) < 2:\n",
    "                    enabled.append(possible_enabled[i])\n",
    "                # if all succeeding events equal all preceding events, we have a flower model and almost everything is enabled all the time\n",
    "                elif filtered_preceding_events[possible_enabled[i]] == filtered_succeeding_activities_updated[\n",
    "                    possible_enabled[i]]:\n",
    "                    enabled.append(possible_enabled[i])\n",
    "                else:\n",
    "                    # if yes, check if all the needed activities have already been performed in this trace\n",
    "                    if all(elem in trace for elem in filtered_preceding_events[possible_enabled[i]]):\n",
    "                        enabled.append(possible_enabled[i])\n",
    "        # extend the list with all elements that do not have any preceding activity and are therefore enabled anyways in our process model\n",
    "        enabled.extend([key for key, value in filtered_preceding_events_full.items() if not value])\n",
    "        # delete all duplicates from the enabled list\n",
    "        enabled = list(set(enabled))\n",
    "    #increate the current trace rat\n",
    "    current_trace_index = current_trace_index + 1\n",
    "    return AG, DG\n",
    "\n",
    "# Define the function that will be executed in parallel\n",
    "def process_group_with(args):\n",
    "    \"\"\"\n",
    "    Function to process a group of the event log in parallel\n",
    "    :param args: set of variables for the measure calculation (see original function)\n",
    "    :return: updated values for AG and DG, type: int\n",
    "    \"\"\"\n",
    "    group_key, df_group, filtered_preceding_events_full, filtered_preceding_events, \\\n",
    "    filtered_succeeding_activities_updated, events, silent_transitions,trace_list, suffix_lists,current_trace_index, AG, DG = args\n",
    "    AG, DG = update_global_variables_with(df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
    "                                     filtered_succeeding_activities_updated, events, silent_transitions,trace_list, suffix_lists,\n",
    "                                          current_trace_index,AG, DG)\n",
    "    return AG, DG\n",
    "\n",
    "\n",
    "def negative_events_with_weighting_parallel(ocel, ocpn):\n",
    "    \"\"\"\n",
    "    Function to calculate the negative events measure with weighting based on the used places inside an object-centric petri-net.\n",
    "    :param ocel: object-centric event log for which the measure should be calculated, type: ocel-log\n",
    "    :param ocpn: corresponding object-centric petri-net, type: object-centric petri-net\n",
    "    :return generalization: final value of the formula, type: float rounded to 4 digits\n",
    "    \"\"\"\n",
    "    # since the process execution mappings have lists of length one,\n",
    "    # we create another dictionary that only contains the the value inside the list to be able to derive the case\n",
    "    mapping_dict = {key: ocel.process_execution_mappings[key][0] for key in ocel.process_execution_mappings}\n",
    "    # we generate a new column in the class (log) that contains the process execution (case) number via the generated dictionary\n",
    "    ocel.log.log['event_execution'] = ocel.log.log.index.map(mapping_dict)\n",
    "    # generate a list of unique events in the event log\n",
    "    events = np.unique(ocel.log.log.event_activity)\n",
    "    # dictionary to store each activity as key and a list of its prior states/places as value\n",
    "    targets = {}\n",
    "    # dictionary to store each activity as key and a list of its following states/places as value\n",
    "    sources = {}\n",
    "    for arc in tqdm(ocpn.arcs, desc=\"Check the arcs\"):\n",
    "        # for each arc, check if our target is a valid transition\n",
    "        if arc.target in ocpn.transitions:\n",
    "            # load all the prior places of a valid transition into a dictionary, where the key is the transition and the value\n",
    "            # a list of all directly prior places\n",
    "            if arc.target.name in targets:\n",
    "                targets[arc.target.name].append(arc.source.name)\n",
    "            else:\n",
    "                targets[arc.target.name] = [arc.source.name]\n",
    "        if arc.source in ocpn.transitions:\n",
    "            # load all the following places of a valid transition into a dictionary, where the key is the transition and the value\n",
    "            # a list of all directly following places\n",
    "            if arc.source.name in sources:\n",
    "                sources[arc.source.name].append(arc.target.name)\n",
    "            else:\n",
    "                sources[arc.source.name] = [arc.target.name]\n",
    "    # generate an empty dictionary to store the directly preceding transition of an activity\n",
    "    preceding_activities = {}\n",
    "    # use the key and value of targets and source to generate the dictionary\n",
    "    for target_key, target_value in targets.items():\n",
    "        preceding_activities[target_key] = []\n",
    "        for source_key, source_value in sources.items():\n",
    "            for element in target_value:\n",
    "                if element in source_value:\n",
    "                    preceding_activities[target_key].append(source_key)\n",
    "                    break\n",
    "    # generate an empty dictionary to store the directly succeeding transition of an activity\n",
    "    succeeding_activities = {}\n",
    "    for source_key, source_value in sources.items():\n",
    "        succeeding_activities[source_key] = []\n",
    "        for target_key, target_value in targets.items():\n",
    "            for element in source_value:\n",
    "                if element in target_value:\n",
    "                    succeeding_activities[source_key].append(target_key)\n",
    "                    break\n",
    "    # store the name of all silent transitions in the log\n",
    "    silent_transitions = [x.name for x in ocpn.transitions if x.silent]\n",
    "    # replace the silent transitions in the succeeding activities dictionary by creating a new dictionary to store the modified values\n",
    "    succeeding_activities_updated = {}\n",
    "    # Iterate through the dictionary\n",
    "    for key, values in succeeding_activities.items():\n",
    "        # Create a list to store the modified values for this key\n",
    "        new_values = []\n",
    "        # Iterate through the values of each key\n",
    "        for i in range(len(values)):\n",
    "            # Check if the value is in the list of silent transitions\n",
    "            if values[i] in silent_transitions:\n",
    "                # Replace the value with the corresponding value from the dictionary\n",
    "                new_values.extend(succeeding_activities[values[i]])\n",
    "            else:\n",
    "                # If the value is not in the list of silent transitions, add it to the new list\n",
    "                new_values.append(values[i])\n",
    "        # Add the modified values to the new dictionary\n",
    "        succeeding_activities_updated[key] = new_values\n",
    "    # create an empty dictionary to store all the preceding activities of an activity\n",
    "    preceding_events_dict = {}\n",
    "    # use a depth-first search (DFS) algorithm to traverse the activity graph and\n",
    "    # create a list of all preceding events for each activity in the dictionary for directly preceding activities\n",
    "    for activity in preceding_activities:\n",
    "        # empty set for all the visited activities\n",
    "        visited = set()\n",
    "        # list for all currently preceding events\n",
    "        preceding_events = []\n",
    "        dfs(preceding_activities, visited, activity, preceding_events)\n",
    "        # we need to remove the last element from the list because it corresponds to the activity itself\n",
    "        preceding_events_dict[activity] = preceding_events[:-1][::-1]\n",
    "    # delete all possible silent transitions from preceding_events_dict (dict where all direct preceding events are stored)\n",
    "    filtered_preceding_events_full = filter_silent_transitions(preceding_events_dict, silent_transitions)\n",
    "    # delete all possible silent transitions from filtered_preceding_events (dict where only direct preceding events are stored)\n",
    "    filtered_preceding_events = filter_silent_transitions(preceding_activities, silent_transitions)\n",
    "    # delete all possible silent transitions from succeeding_activities_updated (dict where only direct preceding events are stored)\n",
    "    filtered_succeeding_activities_updated = filter_silent_transitions(succeeding_activities_updated,\n",
    "                                                                       silent_transitions)\n",
    "    # generate a grouped df such that we can iterate through the log case by case (sort by timestamp to ensure the correct process sequence)\n",
    "    grouped_df = ocel.log.log.sort_values('event_timestamp').groupby('event_execution')\n",
    "    DG = 0  # Disallowed Generalization initialisation\n",
    "    AG = 0  # Allowed Generalization initialisation\n",
    "    #create an empty list for all traces in the log\n",
    "    trace_list = []\n",
    "    for name, group in grouped_df:\n",
    "        values = list(map(str, group['event_activity']))\n",
    "        trace_list.append(values)\n",
    "    #create an empty list for all the reversed suffixes\n",
    "    suffix_lists = []\n",
    "    for activities in trace_list:\n",
    "        #generate the list of suffixes for the reversed traces for more efficient comparison\n",
    "        reversed_activities = activities[::-1]\n",
    "        suffixes = [reversed_activities[i:] for i in range(len(reversed_activities))]\n",
    "        suffix_lists.append(suffixes)\n",
    "    #Initialise the current trace index\n",
    "    return grouped_df, filtered_preceding_events_full, filtered_preceding_events, filtered_succeeding_activities_updated, events, silent_transitions, trace_list, suffix_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c20e2e7",
   "metadata": {},
   "source": [
    "# BPI Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b300869d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Load OCEL Log')\n",
    "ocel = pd.read_pickle('/pfs/data5/home/ma/ma_ma/ma_nsabel/Generalization_in_Object_Centric_Process_Mining/src/data/csv/bpi_log.pickle')\n",
    "\n",
    "\n",
    "with open(\"/pfs/data5/home/ma/ma_ma/ma_nsabel/Generalization_in_Object_Centric_Process_Mining/src/data/csv/bpi_ocpn.pickle\", \"rb\") as file:\n",
    "    ocpn = pickle.load(file)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # generate the variables needed for the parallel processing\n",
    "    grouped_df, filtered_preceding_events_full, filtered_preceding_events, filtered_succeeding_activities_updated, events, silent_transitions, trace_list, suffix_lists = negative_events_with_weighting_parallel(\n",
    "        ocel, ocpn)\n",
    "\n",
    "    current_trace_index = 0  # initialise count variable for trace\n",
    "    DG = 0  # Disallowed Generalization initialisation\n",
    "    AG = 0  # Allowed Generalization initialisation\n",
    "\n",
    "    # Create a multiprocessing Pool\n",
    "    pool = multiprocessing.Pool(10)\n",
    "\n",
    "    # Prepare the arguments for parallel processing\n",
    "    args = [(group_key, df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
    "             filtered_succeeding_activities_updated, events, silent_transitions, trace_list, suffix_lists,\n",
    "             current_trace_index, AG, DG)\n",
    "            for group_key, df_group in grouped_df]\n",
    "\n",
    "    # Apply the parallel processing to each group with additional variables\n",
    "    results = []\n",
    "    with tqdm(total=len(grouped_df)) as pbar:\n",
    "        for result in pool.imap_unordered(process_group_with, args):\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate the final sums of AG and DG\n",
    "    final_AG = sum([result[0] for result in results])\n",
    "    final_DG = sum([result[1] for result in results])\n",
    "\n",
    "    # Close the multiprocessing Pool and join the processes\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # calculate the generalization based on the paper\n",
    "    generalization = final_AG / (final_AG + final_DG)\n",
    "    print(np.round(generalization, 4))\n",
    "\n",
    "    print(\"*** Evaluate ***\")\n",
    "    print(f'The value of generalization for negative events with weights for bpi normal is {generalization}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d910a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*** Negative Events BPI happy**\")\n",
    "\n",
    "\n",
    "\n",
    "with open(\"/pfs/data5/home/ma/ma_ma/ma_nsabel/Generalization_in_Object_Centric_Process_Mining/src/data/csv/bpi_ocpn_happy.pickle\", \"rb\") as file:\n",
    "    ocpn = pickle.load(file)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # generate the variables needed for the parallel processing\n",
    "    grouped_df, filtered_preceding_events_full, filtered_preceding_events, filtered_succeeding_activities_updated, events, silent_transitions, trace_list, suffix_lists = negative_events_with_weighting_parallel(\n",
    "        ocel, ocpn)\n",
    "\n",
    "    current_trace_index = 0  # initialise count variable for trace\n",
    "    DG = 0  # Disallowed Generalization initialisation\n",
    "    AG = 0  # Allowed Generalization initialisation\n",
    "\n",
    "    # Create a multiprocessing Pool\n",
    "    pool = multiprocessing.Pool(10)\n",
    "\n",
    "    # Prepare the arguments for parallel processing\n",
    "    args = [(group_key, df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
    "             filtered_succeeding_activities_updated, events, silent_transitions, trace_list, suffix_lists,\n",
    "             current_trace_index, AG, DG)\n",
    "            for group_key, df_group in grouped_df]\n",
    "\n",
    "    # Apply the parallel processing to each group with additional variables\n",
    "    results = []\n",
    "    with tqdm(total=len(grouped_df)) as pbar:\n",
    "        for result in pool.imap_unordered(process_group_with, args):\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate the final sums of AG and DG\n",
    "    final_AG = sum([result[0] for result in results])\n",
    "    final_DG = sum([result[1] for result in results])\n",
    "\n",
    "    # Close the multiprocessing Pool and join the processes\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # calculate the generalization based on the paper\n",
    "    generalization = final_AG / (final_AG + final_DG)\n",
    "    print(np.round(generalization, 4))\n",
    "\n",
    "    print(\"*** Evaluate ***\")\n",
    "    print(f'The value of generalization for negative events with weights for bpi happy is {generalization}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b786a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*** Negative Events BPI flower**\")\n",
    "\n",
    "\n",
    "\n",
    "with open(\"/pfs/data5/home/ma/ma_ma/ma_nsabel/Generalization_in_Object_Centric_Process_Mining/src/data/csv/bpi_ocpn_flower.pickle\", \"rb\") as file:\n",
    "    ocpn = pickle.load(file)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # generate the variables needed for the parallel processing\n",
    "    grouped_df, filtered_preceding_events_full, filtered_preceding_events, filtered_succeeding_activities_updated, events, silent_transitions, trace_list, suffix_lists = negative_events_with_weighting_parallel(\n",
    "        ocel, ocpn)\n",
    "\n",
    "    current_trace_index = 0  # initialise count variable for trace\n",
    "    DG = 0  # Disallowed Generalization initialisation\n",
    "    AG = 0  # Allowed Generalization initialisation\n",
    "\n",
    "    # Create a multiprocessing Pool\n",
    "    pool = multiprocessing.Pool(10)\n",
    "\n",
    "    # Prepare the arguments for parallel processing\n",
    "    args = [(group_key, df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
    "             filtered_succeeding_activities_updated, events, silent_transitions, trace_list, suffix_lists,\n",
    "             current_trace_index, AG, DG)\n",
    "            for group_key, df_group in grouped_df]\n",
    "\n",
    "    # Apply the parallel processing to each group with additional variables\n",
    "    results = []\n",
    "    with tqdm(total=len(grouped_df)) as pbar:\n",
    "        for result in pool.imap_unordered(process_group_with, args):\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate the final sums of AG and DG\n",
    "    final_AG = sum([result[0] for result in results])\n",
    "    final_DG = sum([result[1] for result in results])\n",
    "\n",
    "    # Close the multiprocessing Pool and join the processes\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # calculate the generalization based on the paper\n",
    "    generalization = final_AG / (final_AG + final_DG)\n",
    "    print(np.round(generalization, 4))\n",
    "\n",
    "    print(\"*** Evaluate ***\")\n",
    "    print(f'The value of generalization for negative events with weights for bpi flower is {generalization}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b3f39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*** Negative Events BPI variant**\")\n",
    "\n",
    "ocel = pd.read_pickle('/pfs/data5/home/ma/ma_ma/ma_nsabel/Generalization_in_Object_Centric_Process_Mining/src/data/csv/bpi_variant.pickle')\n",
    "\n",
    "with open(\"/pfs/data5/home/ma/ma_ma/ma_nsabel/Generalization_in_Object_Centric_Process_Mining/src/data/csv/bpi_variant_ocpn.pickle\", \"rb\") as file:\n",
    "    ocpn = pickle.load(file)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # generate the variables needed for the parallel processing\n",
    "    grouped_df, filtered_preceding_events_full, filtered_preceding_events, filtered_succeeding_activities_updated, events, silent_transitions, trace_list, suffix_lists = negative_events_with_weighting_parallel(\n",
    "        ocel, ocpn)\n",
    "\n",
    "    current_trace_index = 0  # initialise count variable for trace\n",
    "    DG = 0  # Disallowed Generalization initialisation\n",
    "    AG = 0  # Allowed Generalization initialisation\n",
    "\n",
    "    # Create a multiprocessing Pool\n",
    "    pool = multiprocessing.Pool(10)\n",
    "\n",
    "    # Prepare the arguments for parallel processing\n",
    "    args = [(group_key, df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
    "             filtered_succeeding_activities_updated, events, silent_transitions, trace_list, suffix_lists,\n",
    "             current_trace_index, AG, DG)\n",
    "            for group_key, df_group in grouped_df]\n",
    "\n",
    "    # Apply the parallel processing to each group with additional variables\n",
    "    results = []\n",
    "    with tqdm(total=len(grouped_df)) as pbar:\n",
    "        for result in pool.imap_unordered(process_group_with, args):\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate the final sums of AG and DG\n",
    "    final_AG = sum([result[0] for result in results])\n",
    "    final_DG = sum([result[1] for result in results])\n",
    "\n",
    "    # Close the multiprocessing Pool and join the processes\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # calculate the generalization based on the paper\n",
    "    generalization = final_AG / (final_AG + final_DG)\n",
    "    print(np.round(generalization, 4))\n",
    "\n",
    "    print(\"*** Evaluate ***\")\n",
    "    print(f'The value of generalization for negative events with weights for bpi variant is {generalization}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e49cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "162a5b08",
   "metadata": {},
   "source": [
    "# DS3 Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6901d18-78b0-4c6c-b008-1b55d87928bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load OCEL Log\n"
     ]
    }
   ],
   "source": [
    "print('Load OCEL Log')\n",
    "ocel = pd.read_pickle('/pfs/data5/home/ma/ma_ma/ma_nsabel/Generalization_in_Object_Centric_Process_Mining/src/data/csv/DS3_log.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44089548",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(\"/pfs/data5/home/ma/ma_ma/ma_nsabel/Generalization_in_Object_Centric_Process_Mining/src/data/csv/DS3_ocpn.pickle\", \"rb\") as file:\n",
    "    ocpn = pickle.load(file)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # generate the variables needed for the parallel processing\n",
    "    grouped_df, filtered_preceding_events_full, filtered_preceding_events, filtered_succeeding_activities_updated, events, silent_transitions, trace_list, suffix_lists = negative_events_with_weighting_parallel(\n",
    "        ocel, ocpn)\n",
    "\n",
    "    current_trace_index = 0  # initialise count variable for trace\n",
    "    DG = 0  # Disallowed Generalization initialisation\n",
    "    AG = 0  # Allowed Generalization initialisation\n",
    "\n",
    "    # Create a multiprocessing Pool\n",
    "    pool = multiprocessing.Pool(20)\n",
    "\n",
    "    # Prepare the arguments for parallel processing\n",
    "    args = [(group_key, df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
    "             filtered_succeeding_activities_updated, events, silent_transitions, trace_list, suffix_lists,\n",
    "             current_trace_index, AG, DG)\n",
    "            for group_key, df_group in grouped_df]\n",
    "\n",
    "    # Apply the parallel processing to each group with additional variables\n",
    "    results = []\n",
    "    with tqdm(total=len(grouped_df)) as pbar:\n",
    "        for result in pool.imap_unordered(process_group_with, args):\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate the final sums of AG and DG\n",
    "    final_AG = sum([result[0] for result in results])\n",
    "    final_DG = sum([result[1] for result in results])\n",
    "\n",
    "    # Close the multiprocessing Pool and join the processes\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # calculate the generalization based on the paper\n",
    "    generalization = final_AG / (final_AG + final_DG)\n",
    "    print(np.round(generalization, 4))\n",
    "\n",
    "    print(\"*** Evaluate ***\")\n",
    "    print(f'The value of generalization for negative events with weights for ds3 normal is {generalization}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "734f68c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Negative Events DS3 happy**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 8/8 [00:00<00:00, 114520.25it/s]\n",
      "100%|██████████| 4825/4825 [1:40:03<00:00,  1.24s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2022\n",
      "*** Evaluate ***\n",
      "The value of generalization for negative events with weights for ds3 happy is 0.20224624267805857\n"
     ]
    }
   ],
   "source": [
    "print(\"*** Negative Events DS3 happy**\")\n",
    "\n",
    "\n",
    "\n",
    "with open(\"/pfs/data5/home/ma/ma_ma/ma_nsabel/Generalization_in_Object_Centric_Process_Mining/src/data/csv/DS3_ocpn_happy.pickle\", \"rb\") as file:\n",
    "    ocpn = pickle.load(file)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # generate the variables needed for the parallel processing\n",
    "    grouped_df, filtered_preceding_events_full, filtered_preceding_events, filtered_succeeding_activities_updated, events, silent_transitions, trace_list, suffix_lists = negative_events_with_weighting_parallel(\n",
    "        ocel, ocpn)\n",
    "\n",
    "    current_trace_index = 0  # initialise count variable for trace\n",
    "    DG = 0  # Disallowed Generalization initialisation\n",
    "    AG = 0  # Allowed Generalization initialisation\n",
    "\n",
    "    # Create a multiprocessing Pool\n",
    "    pool = multiprocessing.Pool(20)\n",
    "\n",
    "    # Prepare the arguments for parallel processing\n",
    "    args = [(group_key, df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
    "             filtered_succeeding_activities_updated, events, silent_transitions, trace_list, suffix_lists,\n",
    "             current_trace_index, AG, DG)\n",
    "            for group_key, df_group in grouped_df]\n",
    "\n",
    "    # Apply the parallel processing to each group with additional variables\n",
    "    results = []\n",
    "    with tqdm(total=len(grouped_df)) as pbar:\n",
    "        for result in pool.imap_unordered(process_group_with, args):\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate the final sums of AG and DG\n",
    "    final_AG = sum([result[0] for result in results])\n",
    "    final_DG = sum([result[1] for result in results])\n",
    "\n",
    "    # Close the multiprocessing Pool and join the processes\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # calculate the generalization based on the paper\n",
    "    generalization = final_AG / (final_AG + final_DG)\n",
    "    print(np.round(generalization, 4))\n",
    "\n",
    "    print(\"*** Evaluate ***\")\n",
    "    print(f'The value of generalization for negative events with weights for ds3 happy is {generalization}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55171220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Negative Events DS3 flower**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 8/8 [00:00<00:00, 111476.52it/s]\n",
      "100%|██████████| 4825/4825 [1:39:40<00:00,  1.24s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2022\n",
      "*** Evaluate ***\n",
      "The value of generalization for negative events with weights for ds3 flower is 0.20224624267805857\n"
     ]
    }
   ],
   "source": [
    "print(\"*** Negative Events DS3 flower**\")\n",
    "\n",
    "\n",
    "\n",
    "with open(\"/pfs/data5/home/ma/ma_ma/ma_nsabel/Generalization_in_Object_Centric_Process_Mining/src/data/csv/DS3_ocpn_flower.pickle\", \"rb\") as file:\n",
    "    ocpn = pickle.load(file)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # generate the variables needed for the parallel processing\n",
    "    grouped_df, filtered_preceding_events_full, filtered_preceding_events, filtered_succeeding_activities_updated, events, silent_transitions, trace_list, suffix_lists = negative_events_with_weighting_parallel(\n",
    "        ocel, ocpn)\n",
    "\n",
    "    current_trace_index = 0  # initialise count variable for trace\n",
    "    DG = 0  # Disallowed Generalization initialisation\n",
    "    AG = 0  # Allowed Generalization initialisation\n",
    "\n",
    "    # Create a multiprocessing Pool\n",
    "    pool = multiprocessing.Pool(20)\n",
    "\n",
    "    # Prepare the arguments for parallel processing\n",
    "    args = [(group_key, df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
    "             filtered_succeeding_activities_updated, events, silent_transitions, trace_list, suffix_lists,\n",
    "             current_trace_index, AG, DG)\n",
    "            for group_key, df_group in grouped_df]\n",
    "\n",
    "    # Apply the parallel processing to each group with additional variables\n",
    "    results = []\n",
    "    with tqdm(total=len(grouped_df)) as pbar:\n",
    "        for result in pool.imap_unordered(process_group_with, args):\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate the final sums of AG and DG\n",
    "    final_AG = sum([result[0] for result in results])\n",
    "    final_DG = sum([result[1] for result in results])\n",
    "\n",
    "    # Close the multiprocessing Pool and join the processes\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # calculate the generalization based on the paper\n",
    "    generalization = final_AG / (final_AG + final_DG)\n",
    "    print(np.round(generalization, 4))\n",
    "\n",
    "    print(\"*** Evaluate ***\")\n",
    "    print(f'The value of generalization for negative events with weights for ds3 flower is {generalization}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4f44b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*** Negative Events DS3 variant**\")\n",
    "\n",
    "ocel = pd.read_pickle('/pfs/data5/home/ma/ma_ma/ma_nsabel/Generalization_in_Object_Centric_Process_Mining/src/data/csv/ds3_variant.pickle')\n",
    "\n",
    "with open(\"/pfs/data5/home/ma/ma_ma/ma_nsabel/Generalization_in_Object_Centric_Process_Mining/src/data/csv/DS3_variant_ocpn.pickle\", \"rb\") as file:\n",
    "    ocpn = pickle.load(file)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # generate the variables needed for the parallel processing\n",
    "    grouped_df, filtered_preceding_events_full, filtered_preceding_events, filtered_succeeding_activities_updated, events, silent_transitions, trace_list, suffix_lists = negative_events_with_weighting_parallel(\n",
    "        ocel, ocpn)\n",
    "\n",
    "    current_trace_index = 0  # initialise count variable for trace\n",
    "    DG = 0  # Disallowed Generalization initialisation\n",
    "    AG = 0  # Allowed Generalization initialisation\n",
    "\n",
    "    # Create a multiprocessing Pool\n",
    "    pool = multiprocessing.Pool(20)\n",
    "\n",
    "    # Prepare the arguments for parallel processing\n",
    "    args = [(group_key, df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
    "             filtered_succeeding_activities_updated, events, silent_transitions, trace_list, suffix_lists,\n",
    "             current_trace_index, AG, DG)\n",
    "            for group_key, df_group in grouped_df]\n",
    "\n",
    "    # Apply the parallel processing to each group with additional variables\n",
    "    results = []\n",
    "    with tqdm(total=len(grouped_df)) as pbar:\n",
    "        for result in pool.imap_unordered(process_group_with, args):\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate the final sums of AG and DG\n",
    "    final_AG = sum([result[0] for result in results])\n",
    "    final_DG = sum([result[1] for result in results])\n",
    "\n",
    "    # Close the multiprocessing Pool and join the processes\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # calculate the generalization based on the paper\n",
    "    generalization = final_AG / (final_AG + final_DG)\n",
    "    print(np.round(generalization, 4))\n",
    "\n",
    "    print(\"*** Evaluate ***\")\n",
    "    print(f'The value of generalization for negative events with weights for ds3 variant is {generalization}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be461543",
   "metadata": {},
   "source": [
    "# DS4 Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbab0ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load OCEL Log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 364/364 [00:00<00:00, 425627.73it/s]\n",
      "  0%|          | 0/14507 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "print('Load OCEL Log')\n",
    "ocel = pd.read_pickle('/pfs/data5/home/ma/ma_ma/ma_nsabel/Generalization_in_Object_Centric_Process_Mining/src/data/csv/DS4_log.pickle')\n",
    "\n",
    "\n",
    "with open(\"/pfs/data5/home/ma/ma_ma/ma_nsabel/Generalization_in_Object_Centric_Process_Mining/src/data/csv/DS4_ocpn.pickle\", \"rb\") as file:\n",
    "    ocpn = pickle.load(file)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # generate the variables needed for the parallel processing\n",
    "    grouped_df, filtered_preceding_events_full, filtered_preceding_events, filtered_succeeding_activities_updated, events, silent_transitions, trace_list, suffix_lists = negative_events_with_weighting_parallel(\n",
    "        ocel, ocpn)\n",
    "\n",
    "    current_trace_index = 0  # initialise count variable for trace\n",
    "    DG = 0  # Disallowed Generalization initialisation\n",
    "    AG = 0  # Allowed Generalization initialisation\n",
    "\n",
    "    # Create a multiprocessing Pool\n",
    "    pool = multiprocessing.Pool(20)\n",
    "\n",
    "    # Prepare the arguments for parallel processing\n",
    "    args = [(group_key, df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
    "             filtered_succeeding_activities_updated, events, silent_transitions, trace_list, suffix_lists,\n",
    "             current_trace_index, AG, DG)\n",
    "            for group_key, df_group in grouped_df]\n",
    "\n",
    "    # Apply the parallel processing to each group with additional variables\n",
    "    results = []\n",
    "    with tqdm(total=len(grouped_df)) as pbar:\n",
    "        for result in pool.imap_unordered(process_group_with, args):\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate the final sums of AG and DG\n",
    "    final_AG = sum([result[0] for result in results])\n",
    "    final_DG = sum([result[1] for result in results])\n",
    "\n",
    "    # Close the multiprocessing Pool and join the processes\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # calculate the generalization based on the paper\n",
    "    generalization = final_AG / (final_AG + final_DG)\n",
    "    print(np.round(generalization, 4))\n",
    "\n",
    "    print(\"*** Evaluate ***\")\n",
    "    print(f'The value of generalization for negative events with weights for ds4 normal is {generalization}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d546d141",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*** Negative Events DS4 happy**\")\n",
    "\n",
    "\n",
    "\n",
    "with open(\"/pfs/data5/home/ma/ma_ma/ma_nsabel/Generalization_in_Object_Centric_Process_Mining/src/data/csv/DS4_ocpn_happy.pickle\", \"rb\") as file:\n",
    "    ocpn = pickle.load(file)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # generate the variables needed for the parallel processing\n",
    "    grouped_df, filtered_preceding_events_full, filtered_preceding_events, filtered_succeeding_activities_updated, events, silent_transitions, trace_list, suffix_lists = negative_events_with_weighting_parallel(\n",
    "        ocel, ocpn)\n",
    "\n",
    "    current_trace_index = 0  # initialise count variable for trace\n",
    "    DG = 0  # Disallowed Generalization initialisation\n",
    "    AG = 0  # Allowed Generalization initialisation\n",
    "\n",
    "    # Create a multiprocessing Pool\n",
    "    pool = multiprocessing.Pool(20)\n",
    "\n",
    "    # Prepare the arguments for parallel processing\n",
    "    args = [(group_key, df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
    "             filtered_succeeding_activities_updated, events, silent_transitions, trace_list, suffix_lists,\n",
    "             current_trace_index, AG, DG)\n",
    "            for group_key, df_group in grouped_df]\n",
    "\n",
    "    # Apply the parallel processing to each group with additional variables\n",
    "    results = []\n",
    "    with tqdm(total=len(grouped_df)) as pbar:\n",
    "        for result in pool.imap_unordered(process_group_with, args):\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate the final sums of AG and DG\n",
    "    final_AG = sum([result[0] for result in results])\n",
    "    final_DG = sum([result[1] for result in results])\n",
    "\n",
    "    # Close the multiprocessing Pool and join the processes\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # calculate the generalization based on the paper\n",
    "    generalization = final_AG / (final_AG + final_DG)\n",
    "    print(np.round(generalization, 4))\n",
    "\n",
    "    print(\"*** Evaluate ***\")\n",
    "    print(f'The value of generalization for negative events with weights for ds4 happy is {generalization}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e79c972",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*** Negative Events DS4 flower**\")\n",
    "\n",
    "\n",
    "\n",
    "with open(\"/pfs/data5/home/ma/ma_ma/ma_nsabel/Generalization_in_Object_Centric_Process_Mining/src/data/csv/DS4_ocpn_flower.pickle\", \"rb\") as file:\n",
    "    ocpn = pickle.load(file)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # generate the variables needed for the parallel processing\n",
    "    grouped_df, filtered_preceding_events_full, filtered_preceding_events, filtered_succeeding_activities_updated, events, silent_transitions, trace_list, suffix_lists = negative_events_with_weighting_parallel(\n",
    "        ocel, ocpn)\n",
    "\n",
    "    current_trace_index = 0  # initialise count variable for trace\n",
    "    DG = 0  # Disallowed Generalization initialisation\n",
    "    AG = 0  # Allowed Generalization initialisation\n",
    "\n",
    "    # Create a multiprocessing Pool\n",
    "    pool = multiprocessing.Pool(20)\n",
    "\n",
    "    # Prepare the arguments for parallel processing\n",
    "    args = [(group_key, df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
    "             filtered_succeeding_activities_updated, events, silent_transitions, trace_list, suffix_lists,\n",
    "             current_trace_index, AG, DG)\n",
    "            for group_key, df_group in grouped_df]\n",
    "\n",
    "    # Apply the parallel processing to each group with additional variables\n",
    "    results = []\n",
    "    with tqdm(total=len(grouped_df)) as pbar:\n",
    "        for result in pool.imap_unordered(process_group_with, args):\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate the final sums of AG and DG\n",
    "    final_AG = sum([result[0] for result in results])\n",
    "    final_DG = sum([result[1] for result in results])\n",
    "\n",
    "    # Close the multiprocessing Pool and join the processes\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # calculate the generalization based on the paper\n",
    "    generalization = final_AG / (final_AG + final_DG)\n",
    "    print(np.round(generalization, 4))\n",
    "\n",
    "    print(\"*** Evaluate ***\")\n",
    "    print(f'The value of generalization for negative events with weights for ds4 flower is {generalization}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78805aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*** Negative Events DS4 variant**\")\n",
    "\n",
    "ocel = pd.read_pickle('/pfs/data5/home/ma/ma_ma/ma_nsabel/Generalization_in_Object_Centric_Process_Mining/src/data/csv/DS4_variant.pickle')\n",
    "\n",
    "with open(\"/pfs/data5/home/ma/ma_ma/ma_nsabel/Generalization_in_Object_Centric_Process_Mining/src/data/csv/DS4_variant_ocpn.pickle\", \"rb\") as file:\n",
    "    ocpn = pickle.load(file)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # generate the variables needed for the parallel processing\n",
    "    grouped_df, filtered_preceding_events_full, filtered_preceding_events, filtered_succeeding_activities_updated, events, silent_transitions, trace_list, suffix_lists = negative_events_with_weighting_parallel(\n",
    "        ocel, ocpn)\n",
    "\n",
    "    current_trace_index = 0  # initialise count variable for trace\n",
    "    DG = 0  # Disallowed Generalization initialisation\n",
    "    AG = 0  # Allowed Generalization initialisation\n",
    "\n",
    "    # Create a multiprocessing Pool\n",
    "    pool = multiprocessing.Pool(20)\n",
    "\n",
    "    # Prepare the arguments for parallel processing\n",
    "    args = [(group_key, df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
    "             filtered_succeeding_activities_updated, events, silent_transitions, trace_list, suffix_lists,\n",
    "             current_trace_index, AG, DG)\n",
    "            for group_key, df_group in grouped_df]\n",
    "\n",
    "    # Apply the parallel processing to each group with additional variables\n",
    "    results = []\n",
    "    with tqdm(total=len(grouped_df)) as pbar:\n",
    "        for result in pool.imap_unordered(process_group_with, args):\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate the final sums of AG and DG\n",
    "    final_AG = sum([result[0] for result in results])\n",
    "    final_DG = sum([result[1] for result in results])\n",
    "\n",
    "    # Close the multiprocessing Pool and join the processes\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # calculate the generalization based on the paper\n",
    "    generalization = final_AG / (final_AG + final_DG)\n",
    "    print(np.round(generalization, 4))\n",
    "\n",
    "    print(\"*** Evaluate ***\")\n",
    "    print(f'The value of generalization for negative events with weights for ds4 variant is {generalization}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68747995",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
