{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3629febf",
   "metadata": {},
   "source": [
    "# Negative Weighted Events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad07bf88",
   "metadata": {},
   "source": [
    "In this notebook, we adapt the negative events-based measure from van den Broucke et al. from 2014 in the paper: \"Determining Process Model Precision and Generalization with Weighted Artificial Negative Events\" (doi: 10.1109/TKDE.2013.130). <br>\n",
    "With respect to the actual executions of a process, the measure focuses on negative events, where a negative event represents information about activities that were prevented from taking place in the first place. Since they are rarely recorded in reality, we induce them into the log artificially. <br>\n",
    "So, we basically induce all elements that weren't fired at this specific position in the event log. For simplicity, the authors assume that all other events, outside the current event itself, are inserted at each position in the trace. Afterwards, we check which of these events could be fired at the current position in the corresponding position or not. If an event can be fired, we increase a counter for allowed generalizations $AG$ by one, and if not, we increase a counter for allowed generalizations $DG$ by one. <br>\n",
    "The final measure is calculated as follows: <br>\n",
    "For event log $E$ and process model $M$,\n",
    "$$ Generalization (L,M) = AG\\,/\\, (AG+DG).$$\n",
    "One has to be aware, that this is not the complete logic of the paper, which is implemented in a further step by introducing a scoring mechanism. This is done due to the fact that we have to assume the completeness of an event log to induce negative events and this is normally not true in reality, as an event log only represents a subset. The scoring mechanism will allow us to loosen this assumption by only assuming the completeness property on a small window before the execution of the current activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cd21f2c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-16T06:32:40.390994800Z"
    },
    "is_executing": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from ocpa.objects.log.importer.ocel import factory as ocel_import_factory\n",
    "from ocpa.algo.discovery.ocpn import algorithm as ocpn_discovery_factory\n",
    "from src.utils import get_happy_path_log, create_flower_model, generate_variant_model\n",
    "from ocpa.objects.log.importer.csv import factory as ocel_import_factory_csv\n",
    "from models.negative_events_measure import negative_events_without_weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5acca2",
   "metadata": {},
   "source": [
    "# O2C Log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f9f1b6",
   "metadata": {},
   "source": [
    "### Standard Petri Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea029b35",
   "metadata": {},
   "source": [
    "In a first step, we load the OCEL-log into the notebook and generate the object-centric petri net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8448274",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../src/data/jsonocel/order_process.jsonocel\"\n",
    "ocel = ocel_import_factory.apply(filename)\n",
    "ocpn = ocpn_discovery_factory.apply(ocel, parameters={\"debug\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46403402",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 46/46 [00:00<00:00, 46312.53it/s]\n",
      "Calculate Generalization for all process executions: 100%|██████████| 48/48 [00:00<00:00, 170.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5063"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = negative_events_without_weighting (ocel, ocpn)\n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a08e25",
   "metadata": {},
   "source": [
    "### Happy Path Petri Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c4f310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_path__ocel = get_happy_path_log(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35668a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_path_ocpn = ocpn_discovery_factory.apply(happy_path__ocel, parameters={\"debug\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0105752b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 26/26 [00:00<?, ?it/s]\n",
      "Calculate Generalization for all process executions: 100%|██████████| 48/48 [00:00<00:00, 241.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3073"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = negative_events_without_weighting (ocel, happy_path_ocpn)\n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b26a81",
   "metadata": {},
   "source": [
    "### Flower Model Petri Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf08a8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ots = [\"order\",\"item\",\"delivery\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89072314",
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_ocpn = create_flower_model(filename,ots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d01ce1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 32/32 [00:00<?, ?it/s]\n",
      "Calculate Generalization for all process executions: 100%|██████████| 48/48 [00:00<00:00, 223.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.965"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = negative_events_without_weighting (ocel, flower_ocpn)\n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97473e81",
   "metadata": {},
   "source": [
    "### Variant Model Petri Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65f2859",
   "metadata": {},
   "source": [
    "Import the primarily generated variant log for our measure computation, while we generate the variant model with the original log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "808796e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_variant = \"../src/data/csv/order_process_variant_log.csv\" \n",
    "object_types = [\"order\",\"item\",\"delivery\"]\n",
    "parameters = {\"obj_names\": object_types,\n",
    "              \"val_names\": [],\n",
    "              \"act_name\": \"event_activity\",\n",
    "              \"time_name\": \"event_timestamp\",\n",
    "              \"sep\": \",\"}\n",
    "ocel_variant = ocel_import_factory_csv.apply(file_path=filename_variant, parameters=parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50217a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Variant Models: 100%|██████████| 12/12 [00:01<00:00,  7.61it/s]\n",
      "Processing Variant Nets: 100%|██████████| 12/12 [00:00<00:00, 9409.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########Start generating Object-Centric Petri Net#########\n",
      "#########Finished generating Object-Centric Petri Net#########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "filename = \"../src/data/jsonocel/order_process.jsonocel\"\n",
    "ots = [\"order\",\"item\",\"delivery\"]\n",
    "ocel = ocel_import_factory.apply(filename)\n",
    "variant_ocpn = generate_variant_model(ocel,save_path_logs='../src/data/csv/order_variants/order_variant',object_types = ots,save_path_visuals=f\"../reports/figures/order_variant_total.svg\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6da2c58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 378/378 [00:00<00:00, 17582.47it/s]\n",
      "Calculate Generalization for all process executions: 100%|██████████| 48/48 [00:02<00:00, 19.65it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1375"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = negative_events_without_weighting (ocel_variant, variant_ocpn)\n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaab8829",
   "metadata": {},
   "source": [
    "# P2P Log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7816235",
   "metadata": {},
   "source": [
    "### Standard Petri Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8500148a",
   "metadata": {},
   "source": [
    "In a first step, we load the OCEL-log into the notebook and generate the object-centric petri net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "425aa3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../src/data/jsonocel/p2p-normal.jsonocel\"\n",
    "ocel = ocel_import_factory.apply(filename)\n",
    "ocpn = ocpn_discovery_factory.apply(ocel, parameters={\"debug\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b611a86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 40/40 [00:00<00:00, 77065.76it/s]\n",
      "Calculate Generalization for all process executions: 100%|██████████| 80/80 [00:00<00:00, 838.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1845"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = negative_events_without_weighting (ocel, ocpn)\n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a051ca9",
   "metadata": {},
   "source": [
    "### Happy Path Petri Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9910bb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_path__ocel = get_happy_path_log(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84d3b838",
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_path_ocpn = ocpn_discovery_factory.apply(happy_path__ocel, parameters={\"debug\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aedb409b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 38/38 [00:00<00:00, 266974.12it/s]\n",
      "Calculate Generalization for all process executions: 100%|██████████| 80/80 [00:00<00:00, 781.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1958"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = negative_events_without_weighting (ocel, happy_path_ocpn)\n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ec2ccf",
   "metadata": {},
   "source": [
    "### Flower Model Petri Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5deafd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "ots = [\"PURCHORD\",\"INVOICE\",\"PURCHREQ\",\"MATERIAL\",\"GDSRCPT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8caa38bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_ocpn = create_flower_model(filename,ots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b926427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 38/38 [00:00<00:00, 38212.31it/s]\n",
      "Calculate Generalization for all process executions: 100%|██████████| 80/80 [00:00<00:00, 732.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8611"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = negative_events_without_weighting (ocel, flower_ocpn)\n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4b6669",
   "metadata": {},
   "source": [
    "### Variant Model Petri Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298076ae",
   "metadata": {},
   "source": [
    "Import the primarily generated variant log for our measure computation, while we generate the variant model with the original log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4544bcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_variant = \"../src/data/csv/p2p_variant_log.csv\" \n",
    "object_types = [\"PURCHORD\",\"INVOICE\",\"PURCHREQ\",\"MATERIAL\",\"GDSRCPT\"]\n",
    "parameters = {\"obj_names\": object_types,\n",
    "              \"val_names\": [],\n",
    "              \"act_name\": \"event_activity\",\n",
    "              \"time_name\": \"event_timestamp\",\n",
    "              \"sep\": \",\"}\n",
    "ocel_variant = ocel_import_factory_csv.apply(file_path=filename_variant, parameters=parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d3b946c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Variant Models: 100%|██████████| 20/20 [00:02<00:00,  9.98it/s]\n",
      "Processing Variant Nets: 100%|██████████| 20/20 [00:00<00:00, 9618.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########Start generating Object-Centric Petri Net#########\n",
      "#########Finished generating Object-Centric Petri Net#########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "filename = \"../src/data/jsonocel/p2p-normal.jsonocel\"\n",
    "ots = [\"PURCHORD\",\"INVOICE\",\"PURCHREQ\",\"MATERIAL\",\"GDSRCPT\"]\n",
    "ocel = ocel_import_factory.apply(filename)\n",
    "variant_ocpn = generate_variant_model(ocel,save_path_logs='../src/data/csv/p2p_variants/p2p_variant',object_types = ots ,save_path_visuals=f\"../reports/figures/p2p_variant_total.svg\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ec778a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 760/760 [00:00<00:00, 21810.51it/s]\n",
      "Calculate Generalization for all process executions: 100%|██████████| 80/80 [00:00<00:00, 80.15it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.115"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = negative_events_without_weighting (ocel_variant, variant_ocpn)\n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406b591f",
   "metadata": {},
   "source": [
    "# BPI-Challenge 2017 Log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef9fbd0",
   "metadata": {},
   "source": [
    "### Standard Petri Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f37d90",
   "metadata": {},
   "source": [
    "In a first step, we load the OCEL-log into the notebook and generate the object-centric petri net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97d78201",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../src/data/jsonocel/BPI2017-Final.jsonocel\"\n",
    "ocel = ocel_import_factory.apply(filename)\n",
    "ocpn = ocpn_discovery_factory.apply(ocel, parameters={\"debug\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24ab8753",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 120/120 [00:00<00:00, 118650.75it/s]\n",
      "Calculate Generalization for all process executions: 100%|██████████| 31509/31509 [01:00<00:00, 524.52it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3569"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = negative_events_without_weighting (ocel, ocpn)\n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f22ca5",
   "metadata": {},
   "source": [
    "### Happy Path Petri Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da118f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_path__ocel = get_happy_path_log(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d570317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_path_ocpn = ocpn_discovery_factory.apply(happy_path__ocel, parameters={\"debug\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9582914a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 26/26 [00:00<00:00, 24363.70it/s]\n",
      "Calculate Generalization for all process executions: 100%|██████████| 31509/31509 [01:00<00:00, 522.93it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1077"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = negative_events_without_weighting (ocel, happy_path_ocpn)\n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a62a8d3",
   "metadata": {},
   "source": [
    "### Flower Model Petri Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69ddb486",
   "metadata": {},
   "outputs": [],
   "source": [
    "ots = [\"application\",\"offer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a60d3fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_ocpn = create_flower_model(filename,ots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca71b5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 56/56 [00:00<00:00, 19040.29it/s]\n",
      "Calculate Generalization for all process executions: 100%|██████████| 31509/31509 [01:04<00:00, 487.77it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8827"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = negative_events_without_weighting (ocel, flower_ocpn)\n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15c62c4-3553-42f8-9fd0-d0c71890f480",
   "metadata": {},
   "source": [
    "# Run parallelism for variant models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4246124-2b41-4944-b90d-b1ba2cbc156b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import multiprocessing\n",
    "def filter_silent_transitions(dic,silent_transitions):\n",
    "    \"\"\"\n",
    "    Function to filter out the silent transitions defined by a list from a given dictionary.\n",
    "    :param dic: dictionary to be filtered, type: dictionary\n",
    "    :param silent_transitions: list of silent transitions in an ocel log, type: list\n",
    "    :return updated_dictionary: filtered dictionary, type: dictionary\n",
    "    \"\"\"\n",
    "    updated_dictionary = {}\n",
    "    for key, values in dic.items():\n",
    "        if key not in silent_transitions:\n",
    "            new_values = [val for val in values if val not in silent_transitions]\n",
    "            updated_dictionary[key] = new_values\n",
    "    return updated_dictionary\n",
    "\n",
    "#recursive implementation of a depth-first search (DFS) algorithm\n",
    "def dfs(graph, visited, activity, preceding_events):\n",
    "    \"\"\"\n",
    "    Function to perform a depth-first search (DFS) algorithm on the activity graph.\n",
    "    :param graph: activity graph, type: dictionary\n",
    "    :param visited: set of already visited nodes, type: set\n",
    "    :param activity: current activity, type: string\n",
    "    :param preceding_events: list to store the preceding events, type: list\n",
    "    \"\"\"\n",
    "    #takes as input the activity graph (represented as a dictionary), a set of visited nodes, the current activity, and a list to store the preceding events.\n",
    "    visited.add(activity)\n",
    "    for preceding_event in graph[activity]:\n",
    "        #eighboring activity has not been visited yet, the algorithm visits it by calling the dfs function with the neighboring activity as the current activity.\n",
    "        if preceding_event not in visited:\n",
    "            dfs(graph, visited, preceding_event, preceding_events)\n",
    "    preceding_events.append(activity)\n",
    "\n",
    "\n",
    "def update_global_variables_without(group, filtered_preceding_events_full, filtered_preceding_events,\n",
    "                            filtered_succeeding_activities_updated, events, silent_transitions, AG, DG):\n",
    "    \"\"\"\n",
    "    Function to update the global variables AG and DG based on the paper logic.\n",
    "    :param group: group of the event log that is currently processed, type: pandas dataframe\n",
    "    :param filtered_preceding_events_full: dictionary that contains all preceding events for each activity, type: dictionary\n",
    "    :param filtered_preceding_events: dictionary that contains all directly preceding events for each activity, type: dictionary\n",
    "    :param filtered_succeeding_activities_updated: dictionary that contains all succeeding activities for each activity, type: dictionary\n",
    "    :param events: list of all activities in the process model, type: list\n",
    "    :param silent_transitions: list of all silent transitions in the process model, type: list\n",
    "    :param AG: global variable for allowed generalizations, type: int\n",
    "    :param DG: global variable for disallowed generalizations, type: int\n",
    "    :return: updated values for AG and DG, type: int\n",
    "    \"\"\"\n",
    "    # Iterate over each row in the group\n",
    "    # list for all the activities that are enabled, starting from all activities that do not have any preceding activity\n",
    "    enabled = [key for key, value in filtered_preceding_events_full.items() if not value]\n",
    "    # initialise a list of already executed activities in this trace\n",
    "    trace = []\n",
    "    # iterate through each case/process execution\n",
    "    for index, row in group.iterrows():\n",
    "        # Get the current negative events based on the current activity to be executed\n",
    "        negative_activities = [x for x in events if x != row['event_activity']]\n",
    "        # it may happen that an activity is not present in the model but nevertheless executed in the log\n",
    "        if row['event_activity'] in enabled:\n",
    "            # check which elements in the negative activity list are enabled outside of the current activity\n",
    "            enabled.remove(row['event_activity'])\n",
    "        # get all the negative events that can not be executed in the process model at the moment\n",
    "        disallowed = [value for value in negative_activities if value not in enabled]\n",
    "        # add activity that has been executed to trace\n",
    "        trace.append(row['event_activity'])\n",
    "        # update the values of allowed and disallowed generalizations based on the paper logic\n",
    "        AG = AG + len(enabled)\n",
    "        DG = DG + len(disallowed)\n",
    "\n",
    "        # may happen that activities in the log are not in the process model\n",
    "        if row['event_activity'] in filtered_succeeding_activities_updated:\n",
    "            # get all possible new enabled activities\n",
    "            possible_enabled = filtered_succeeding_activities_updated[row['event_activity']]\n",
    "            # check if each activity has more than one directly preceding state\n",
    "            for i in range(len(possible_enabled)):\n",
    "                # check if an event has two or more activities that need to be executed before the event can take place, if not add events to enabled\n",
    "                if len(filtered_preceding_events[possible_enabled[i]]) < 2:\n",
    "                    enabled.append(possible_enabled[i])\n",
    "                # if all succeeding events equal all preceding events, we have a flower model and almost everything is enabled all the time\n",
    "                elif filtered_preceding_events[possible_enabled[i]] == filtered_succeeding_activities_updated[\n",
    "                    possible_enabled[i]]:\n",
    "                    enabled.append(possible_enabled[i])\n",
    "                else:\n",
    "                    # if yes, check if all the needed activities have already been performed in this trace\n",
    "                    if all(elem in trace for elem in filtered_preceding_events[possible_enabled[i]]):\n",
    "                        enabled.append(possible_enabled[i])\n",
    "        # extend the list with all elements that do not have any preceding activity and are therefore enabled anyways in our process model\n",
    "        enabled.extend([key for key, value in filtered_preceding_events_full.items() if not value])\n",
    "        # delete all duplicates from the enabled list\n",
    "        enabled = list(set(enabled))\n",
    "    return AG, DG\n",
    "\n",
    "# Define the function that will be executed in parallel\n",
    "def process_group_without(args):\n",
    "    \"\"\"\n",
    "    Function to process a group of the event log in parallel\n",
    "    :param args: set of variables for the measure calculation (see original function)\n",
    "    :return: updated values for AG and DG, type: int\n",
    "    \"\"\"\n",
    "    group_key, df_group, filtered_preceding_events_full, filtered_preceding_events, \\\n",
    "    filtered_succeeding_activities_updated, events, silent_transitions, AG, DG = args\n",
    "    AG, DG = update_global_variables_without(df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
    "                                     filtered_succeeding_activities_updated, events, silent_transitions, AG, DG)\n",
    "    return AG, DG\n",
    "\n",
    "def negative_events_without_weighting_parallel(ocel, ocpn):\n",
    "    \"\"\"\n",
    "    Function to calculate the variables for negative events measure paralle calculation without weighting\n",
    "    :param ocel: object-centric event log for which the measure should be calculated, type: ocel-log\n",
    "    :param ocpn: corresponding object-centric petri-net, type: object-centric petri-net\n",
    "    :return: set of variables for the measure calculation (see original function)\n",
    "    \"\"\"\n",
    "    # since the process execution mappings have lists of length one,\n",
    "    # we create another dictionary that only contains the  value inside the list to be able to derive the case\n",
    "    mapping_dict = {key: ocel.process_execution_mappings[key][0] for key in ocel.process_execution_mappings}\n",
    "    # we generate a new column in the class (log) that contains the process execution (case) number via the generated dictionary\n",
    "    ocel.log.log['event_execution'] = ocel.log.log.index.map(mapping_dict)\n",
    "    # generate a list of unique events in the event log\n",
    "    events = np.unique(ocel.log.log.event_activity)\n",
    "    # dictionary to store each activity as key and a list of its prior states/places as value\n",
    "    targets = {}\n",
    "    # dictionary to store each activity as key and a list of its following states/places as value\n",
    "    sources = {}\n",
    "    for arc in tqdm(ocpn.arcs, desc=\"Check the arcs\"):\n",
    "        # for each arc, check if our target is a valid transition\n",
    "        if arc.target in ocpn.transitions:\n",
    "            # load all the prior places of a valid transition into a dictionary, where the key is the transition and the value\n",
    "            # a list of all directly prior places\n",
    "            if arc.target.name in targets:\n",
    "                targets[arc.target.name].append(arc.source.name)\n",
    "            else:\n",
    "                targets[arc.target.name] = [arc.source.name]\n",
    "        if arc.source in ocpn.transitions:\n",
    "            # load all the following places of a valid transition into a dictionary, where the key is the transition and the value\n",
    "            # a list of all directly following places\n",
    "            if arc.source.name in sources:\n",
    "                sources[arc.source.name].append(arc.target.name)\n",
    "            else:\n",
    "                sources[arc.source.name] = [arc.target.name]\n",
    "    # generate an empty dictionary to store the directly preceding transition of an activity\n",
    "    preceding_activities = {}\n",
    "    # use the key and value of targets and source to generate the dictionary\n",
    "    for target_key, target_value in targets.items():\n",
    "        preceding_activities[target_key] = []\n",
    "        for source_key, source_value in sources.items():\n",
    "            for element in target_value:\n",
    "                if element in source_value:\n",
    "                    preceding_activities[target_key].append(source_key)\n",
    "                    break\n",
    "    # generate an empty dictionary to store the directly succeeding transition of an activity\n",
    "    succeeding_activities = {}\n",
    "    for source_key, source_value in sources.items():\n",
    "        succeeding_activities[source_key] = []\n",
    "        for target_key, target_value in targets.items():\n",
    "            for element in source_value:\n",
    "                if element in target_value:\n",
    "                    succeeding_activities[source_key].append(target_key)\n",
    "                    break\n",
    "    # store the name of all silent transitions in the log\n",
    "    silent_transitions = [x.name for x in ocpn.transitions if x.silent]\n",
    "    # replace the silent transitions in the succeeding activities dictionary by creating a new dictionary to store the modified values\n",
    "    succeeding_activities_updated = {}\n",
    "    # Iterate through the dictionary\n",
    "    for key, values in succeeding_activities.items():\n",
    "        # Create a list to store the modified values for this key\n",
    "        new_values = []\n",
    "        # Iterate through the values of each key\n",
    "        for i in range(len(values)):\n",
    "            # Check if the value is in the list of silent transitions\n",
    "            if values[i] in silent_transitions:\n",
    "                # Replace the value with the corresponding value from the dictionary\n",
    "                new_values.extend(succeeding_activities[values[i]])\n",
    "            else:\n",
    "                # If the value is not in the list of silent transitions, add it to the new list\n",
    "                new_values.append(values[i])\n",
    "        # Add the modified values to the new dictionary\n",
    "        succeeding_activities_updated[key] = new_values\n",
    "    # create an empty dictionary to store all the preceding activities of an activity\n",
    "    preceding_events_dict = {}\n",
    "    # use a depth-first search (DFS) algorithm to traverse the activity graph and\n",
    "    # create a list of all preceding events for each activity in the dictionary for directly preceding activities\n",
    "    for activity in preceding_activities:\n",
    "        # empty set for all the visited activities\n",
    "        visited = set()\n",
    "        # list for all currently preceding events\n",
    "        preceding_events = []\n",
    "        dfs(preceding_activities, visited, activity, preceding_events)\n",
    "        # we need to remove the last element from the list because it corresponds to the activity itself\n",
    "        preceding_events_dict[activity] = preceding_events[:-1][::-1]\n",
    "    # delete all possible silent transitions from preceding_events_dict (dict where all direct preceding events are stored)\n",
    "    filtered_preceding_events_full = filter_silent_transitions(preceding_events_dict, silent_transitions)\n",
    "    # delete all possible silent transitions from filtered_preceding_events (dict where only direct preceding events are stored)\n",
    "    filtered_preceding_events = filter_silent_transitions(preceding_activities, silent_transitions)\n",
    "    # delete all possible silent transitions from succeeding_activities_updated (dict where only direct preceding events are stored)\n",
    "    filtered_succeeding_activities_updated = filter_silent_transitions(succeeding_activities_updated,\n",
    "                                                                       silent_transitions)\n",
    "    # generate a grouped df such that we can iterate through the log case by case (sort by timestamp to ensure the correct process sequence)\n",
    "    grouped_df = ocel.log.log.sort_values('event_timestamp').groupby('event_execution')\n",
    "\n",
    "    return grouped_df, filtered_preceding_events_full, filtered_preceding_events, filtered_succeeding_activities_updated, events, silent_transitions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a026f0",
   "metadata": {},
   "source": [
    "### Variant Model Petri Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c934c956",
   "metadata": {},
   "source": [
    "We import the pickle file for the variant model of the bpi challenge that we generated in the process models notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fb505b2-9b9c-4c35-bdb4-024cf5a34e18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "variant_ocel = pd.read_pickle('/pfs/data5/home/ma/ma_ma/ma_nsabel/Generalization_in_Object_Centric_Process_Mining/src/data/csv/bpi_variant.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bc14dd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"../src/data/csv/bpi_variant_ocpn.pickle\", \"rb\") as file:\n",
    "    variant_ocpn = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76eb9de3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 214724/214724 [1:01:08<00:00, 58.53it/s]\n",
      "  1%|          | 219/31509 [42:54<102:11:14, 11.76s/it]Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-20:\n",
      "Process ForkPoolWorker-9:\n",
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-16:\n",
      "Process ForkPoolWorker-15:\n",
      "Process ForkPoolWorker-10:\n",
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-13:\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-17:\n",
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-18:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-11:\n",
      "Process ForkPoolWorker-19:\n",
      "Process ForkPoolWorker-14:\n",
      "Process ForkPoolWorker-12:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "File \u001B[0;32m/usr/lib64/python3.9/multiprocessing/pool.py:853\u001B[0m, in \u001B[0;36mIMapIterator.next\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    852\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 853\u001B[0m     item \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_items\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpopleft\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    854\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m:\n",
      "\u001B[0;31mIndexError\u001B[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 21\u001B[0m\n\u001B[1;32m     19\u001B[0m results \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tqdm(total\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(grouped_df)) \u001B[38;5;28;01mas\u001B[39;00m pbar:\n\u001B[0;32m---> 21\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m result \u001B[38;5;129;01min\u001B[39;00m pool\u001B[38;5;241m.\u001B[39mimap_unordered(process_group_without, args):\n\u001B[1;32m     22\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(result)\n\u001B[1;32m     23\u001B[0m         pbar\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m/usr/lib64/python3.9/multiprocessing/pool.py:858\u001B[0m, in \u001B[0;36mIMapIterator.next\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    856\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pool \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    857\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 858\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cond\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    859\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    860\u001B[0m     item \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_items\u001B[38;5;241m.\u001B[39mpopleft()\n",
      "File \u001B[0;32m/usr/lib64/python3.9/threading.py:312\u001B[0m, in \u001B[0;36mCondition.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    310\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:    \u001B[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[39;00m\n\u001B[1;32m    311\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 312\u001B[0m         \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    313\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    314\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 109, in process_group_without\n",
      "    AG, DG = update_global_variables_without(df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 109, in process_group_without\n",
      "    AG, DG = update_global_variables_without(df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 109, in process_group_without\n",
      "    AG, DG = update_global_variables_without(df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 109, in process_group_without\n",
      "    AG, DG = update_global_variables_without(df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 109, in process_group_without\n",
      "    AG, DG = update_global_variables_without(df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 109, in process_group_without\n",
      "    AG, DG = update_global_variables_without(df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 109, in process_group_without\n",
      "    AG, DG = update_global_variables_without(df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 109, in process_group_without\n",
      "    AG, DG = update_global_variables_without(df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 109, in process_group_without\n",
      "    AG, DG = update_global_variables_without(df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 109, in process_group_without\n",
      "    AG, DG = update_global_variables_without(df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 109, in process_group_without\n",
      "    AG, DG = update_global_variables_without(df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 109, in process_group_without\n",
      "    AG, DG = update_global_variables_without(df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 109, in process_group_without\n",
      "    AG, DG = update_global_variables_without(df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 109, in process_group_without\n",
      "    AG, DG = update_global_variables_without(df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in update_global_variables_without\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 109, in process_group_without\n",
      "    AG, DG = update_global_variables_without(df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 109, in process_group_without\n",
      "    AG, DG = update_global_variables_without(df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 109, in process_group_without\n",
      "    AG, DG = update_global_variables_without(df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in update_global_variables_without\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 109, in process_group_without\n",
      "    AG, DG = update_global_variables_without(df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in update_global_variables_without\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in update_global_variables_without\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in update_global_variables_without\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in update_global_variables_without\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in update_global_variables_without\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in update_global_variables_without\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in update_global_variables_without\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in update_global_variables_without\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in update_global_variables_without\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in update_global_variables_without\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in <listcomp>\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in update_global_variables_without\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in update_global_variables_without\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in update_global_variables_without\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in update_global_variables_without\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in <listcomp>\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in update_global_variables_without\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in <listcomp>\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in <listcomp>\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in <listcomp>\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in <listcomp>\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in <listcomp>\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in <listcomp>\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in update_global_variables_without\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in <listcomp>\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in <listcomp>\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in <listcomp>\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "KeyboardInterrupt\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in <listcomp>\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in <listcomp>\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in <listcomp>\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in <listcomp>\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in <listcomp>\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "KeyboardInterrupt\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in <listcomp>\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 70, in <listcomp>\n",
      "    disallowed = [value for value in negative_activities if value not in enabled]\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 109, in process_group_without\n",
      "    AG, DG = update_global_variables_without(df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 64, in update_global_variables_without\n",
      "    negative_activities = [x for x in events if x != row['event_activity']]\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 64, in <listcomp>\n",
      "    negative_activities = [x for x in events if x != row['event_activity']]\n",
      "  File \"/home/ma/ma_ma/ma_nsabel/.local/lib/python3.9/site-packages/pandas/core/series.py\", line 1012, in __getitem__\n",
      "    return self._get_value(key)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ma/ma_ma/ma_nsabel/.local/lib/python3.9/site-packages/pandas/core/series.py\", line 1124, in _get_value\n",
      "    return self._values[loc]\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 109, in process_group_without\n",
      "    AG, DG = update_global_variables_without(df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 64, in update_global_variables_without\n",
      "    negative_activities = [x for x in events if x != row['event_activity']]\n",
      "  File \"/scratch/slurm_tmpdir/job_22364389/ipykernel_671705/3566376970.py\", line 64, in <listcomp>\n",
      "    negative_activities = [x for x in events if x != row['event_activity']]\n",
      "  File \"/home/ma/ma_ma/ma_nsabel/.local/lib/python3.9/site-packages/pandas/core/series.py\", line 1012, in __getitem__\n",
      "    return self._get_value(key)\n",
      "  File \"/home/ma/ma_ma/ma_nsabel/.local/lib/python3.9/site-packages/pandas/core/series.py\", line 1121, in _get_value\n",
      "    loc = self.index.get_loc(label)\n",
      "  File \"/home/ma/ma_ma/ma_nsabel/.local/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 3652, in get_loc\n",
      "    return self._engine.get_loc(casted_key)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    #generate the variables needed for the parallel processing\n",
    "    grouped_df, filtered_preceding_events_full, filtered_preceding_events, filtered_succeeding_activities_updated, events, silent_transitions = negative_events_without_weighting_parallel(variant_ocel, variant_ocpn)\n",
    "\n",
    "    DG = 0  # Disallowed Generalization initialisation\n",
    "    AG = 0  # Allowed Generalization initialisation\n",
    "\n",
    "    # Create a multiprocessing Pool\n",
    "    pool = multiprocessing.Pool(20)\n",
    "\n",
    "    # Prepare the arguments for parallel processing\n",
    "    args = [(group_key, df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
    "             filtered_succeeding_activities_updated, events, silent_transitions, AG, DG)\n",
    "            for group_key, df_group in grouped_df]\n",
    "\n",
    "    \n",
    "    # Apply the parallel processing to each group with additional variables\n",
    "    results = []\n",
    "    with tqdm(total=len(grouped_df)) as pbar:\n",
    "        for result in pool.imap_unordered(process_group_without, args):\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate the final sums of AG and DG\n",
    "    final_AG = sum([result[0] for result in results])\n",
    "    final_DG = sum([result[1] for result in results])\n",
    "\n",
    "    # Close the multiprocessing Pool and join the processes\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # calculate the generalization based on the paper\n",
    "    generalization = final_AG / (final_AG + final_DG)\n",
    "    print(np.round(generalization,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c17e95",
   "metadata": {},
   "source": [
    "# DS3 Log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719c545d",
   "metadata": {},
   "source": [
    "### Standard Petri Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2f6040",
   "metadata": {},
   "source": [
    "In a first step, we load the OCEL-log into the notebook and generate the object-centric petri net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ec14354",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../src/data/jsonocel/DS3.jsonocel\"\n",
    "ocel = ocel_import_factory.apply(filename)\n",
    "ocpn = ocpn_discovery_factory.apply(ocel, parameters={\"debug\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63ac4fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 130/130 [00:00<?, ?it/s]\n",
      "Calculate Generalization for all process executions: 100%|██████████| 4825/4825 [00:11<00:00, 405.91it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3701"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = negative_events_without_weighting(ocel,ocpn)\n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8e2440",
   "metadata": {},
   "source": [
    "### Happy Path Petri Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57ddcb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_path__ocel = get_happy_path_log(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ed05767",
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_path_ocpn = ocpn_discovery_factory.apply(happy_path__ocel, parameters={\"debug\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8834e645",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 8/8 [00:00<?, ?it/s]\n",
      "Calculate Generalization for all process executions: 100%|██████████| 4825/4825 [00:12<00:00, 395.43it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2073"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = negative_events_without_weighting(ocel,happy_path_ocpn)\n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10163de1",
   "metadata": {},
   "source": [
    "### Flower Model Petri Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c47d366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ots = [\"incident\",\"customer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e99f3f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_ocpn = create_flower_model(filename,ots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4be633d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 20/20 [00:00<?, ?it/s]\n",
      "Calculate Generalization for all process executions: 100%|██████████| 4825/4825 [00:12<00:00, 373.28it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9588"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = negative_events_without_weighting(ocel,flower_ocpn)\n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5696dc73",
   "metadata": {},
   "source": [
    "### Variant Model Petri Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69785b52",
   "metadata": {},
   "source": [
    "Import the primarily generated variant log for our measure computation, while we generate the variant model with the original log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955b7082-b3da-4248-baa5-c868182b2317",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_ocel = pd.read_pickle('/pfs/data5/home/ma/ma_ma/ma_nsabel/Generalization_in_Object_Centric_Process_Mining/src/data/csv/ds3_variant.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2db29935",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../src/data/csv/DS3_variant_ocpn.pickle\", \"rb\") as file:\n",
    "    variant_ocpn = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43d747ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs:  67%|██████▋   | 98030/146804 [47:10<29:01, 28.00it/s]  "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    #generate the variables needed for the parallel processing\n",
    "    grouped_df, filtered_preceding_events_full, filtered_preceding_events, filtered_succeeding_activities_updated, events, silent_transitions = negative_events_without_weighting_parallel(variant_ocel, variant_ocpn)\n",
    "\n",
    "    DG = 0  # Disallowed Generalization initialisation\n",
    "    AG = 0  # Allowed Generalization initialisation\n",
    "\n",
    "    # Create a multiprocessing Pool\n",
    "    pool = multiprocessing.Pool(20)\n",
    "\n",
    "    # Prepare the arguments for parallel processing\n",
    "    args = [(group_key, df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
    "             filtered_succeeding_activities_updated, events, silent_transitions, AG, DG)\n",
    "            for group_key, df_group in grouped_df]\n",
    "\n",
    "    \n",
    "    # Apply the parallel processing to each group with additional variables\n",
    "    results = []\n",
    "    with tqdm(total=len(grouped_df)) as pbar:\n",
    "        for result in pool.imap_unordered(process_group_without, args):\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate the final sums of AG and DG\n",
    "    final_AG = sum([result[0] for result in results])\n",
    "    final_DG = sum([result[1] for result in results])\n",
    "\n",
    "    # Close the multiprocessing Pool and join the processes\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # calculate the generalization based on the paper\n",
    "    generalization = final_AG / (final_AG + final_DG)\n",
    "    print(np.round(generalization,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f126e6",
   "metadata": {},
   "source": [
    "# DS4 Log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018e20d0",
   "metadata": {},
   "source": [
    "### Standard Petri Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c50fd73",
   "metadata": {},
   "source": [
    "In a first step, we load the OCEL-log into the notebook and generate the object-centric petri net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0492c9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../src/data/jsonocel/DS4.jsonocel\"\n",
    "ocel = ocel_import_factory.apply(filename)\n",
    "ocpn = ocpn_discovery_factory.apply(ocel, parameters={\"debug\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3a346fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 364/364 [00:00<00:00, 168271.43it/s]\n",
      "Calculate Generalization for all process executions: 100%|██████████| 14507/14507 [05:39<00:00, 42.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3604"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = negative_events_without_weighting(ocel,ocpn)\n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1437d2",
   "metadata": {},
   "source": [
    "### Happy Path Petri Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4f5ffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_path__ocel = get_happy_path_log(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e524ae81",
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_path_ocpn = ocpn_discovery_factory.apply(happy_path__ocel, parameters={\"debug\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d14a533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 70/70 [00:00<00:00, 72512.05it/s]\n",
      "Calculate Generalization for all process executions: 100%|██████████| 14507/14507 [05:07<00:00, 47.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0842"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = negative_events_without_weighting(ocel,happy_path_ocpn)\n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcccf11",
   "metadata": {},
   "source": [
    "### Flower Model Petri Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c197b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "ots = [\"Payment application\",\"Control summary\",\"Entitlement application\",\"Geo parcel document\",\"Inspection\",\"Reference alignment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f84ac84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_ocpn = create_flower_model(filename,ots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dd82f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check the arcs: 100%|██████████| 162/162 [00:00<00:00, 23182.44it/s]\n",
      "Calculate Generalization for all process executions: 100%|██████████| 14507/14507 [18:09<00:00, 13.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6885"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = negative_events_without_weighting(ocel,flower_ocpn)\n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c09b15",
   "metadata": {},
   "source": [
    "### Variant Model Petri Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10a892b",
   "metadata": {},
   "source": [
    "Import the primarily generated variant log for our measure computation, while we generate the variant model with the original log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a29a7dc-c875-4743-8cac-1b420846f5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_ocel = pd.read_pickle('/pfs/data5/home/ma/ma_ma/ma_nsabel/Generalization_in_Object_Centric_Process_Mining/src/data/csv/DS4_variant.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d30e292",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../src/data/csv/DS4_variant_ocpn.pickle\", \"rb\") as file:\n",
    "    variant_ocpn = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff4017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    #generate the variables needed for the parallel processing\n",
    "    grouped_df, filtered_preceding_events_full, filtered_preceding_events, filtered_succeeding_activities_updated, events, silent_transitions = negative_events_without_weighting_parallel(variant_ocel, variant_ocpn)\n",
    "\n",
    "    DG = 0  # Disallowed Generalization initialisation\n",
    "    AG = 0  # Allowed Generalization initialisation\n",
    "\n",
    "    # Create a multiprocessing Pool\n",
    "    pool = multiprocessing.Pool(20)\n",
    "\n",
    "    # Prepare the arguments for parallel processing\n",
    "    args = [(group_key, df_group, filtered_preceding_events_full, filtered_preceding_events,\n",
    "             filtered_succeeding_activities_updated, events, silent_transitions, AG, DG)\n",
    "            for group_key, df_group in grouped_df]\n",
    "\n",
    "    \n",
    "    # Apply the parallel processing to each group with additional variables\n",
    "    results = []\n",
    "    with tqdm(total=len(grouped_df)) as pbar:\n",
    "        for result in pool.imap_unordered(process_group_without, args):\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate the final sums of AG and DG\n",
    "    final_AG = sum([result[0] for result in results])\n",
    "    final_DG = sum([result[1] for result in results])\n",
    "\n",
    "    # Close the multiprocessing Pool and join the processes\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # calculate the generalization based on the paper\n",
    "    generalization = final_AG / (final_AG + final_DG)\n",
    "    print(np.round(generalization,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d59990",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
